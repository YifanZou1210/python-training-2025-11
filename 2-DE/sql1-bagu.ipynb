{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054bee1c",
   "metadata": {},
   "source": [
    "# SQL BAGU \n",
    "\n",
    "## **Why single-node databases can’t handle Big Data**\n",
    "\n",
    "* **Storage limit:** Single-node systems have finite disk and memory.\n",
    "* **Compute limit:** Single CPU/machine cannot efficiently process petabytes of data.\n",
    "* **Fault tolerance:** Single-node failure causes complete downtime.\n",
    "\n",
    "---\n",
    "\n",
    "## **Problems solved by distributed systems & new challenges**\n",
    "\n",
    "* **Solves:**\n",
    "\n",
    "  * Horizontal scalability\n",
    "  * Fault tolerance / high availability\n",
    "  * Parallel processing for large datasets\n",
    "\n",
    "* **Challenges:**\n",
    "\n",
    "  * Network latency\n",
    "  * Data consistency (CAP theorem)\n",
    "  * Distributed transactions\n",
    "  * Debugging and monitoring complexity\n",
    "\n",
    "---\n",
    "\n",
    "## **OLTP vs OLAP**\n",
    "\n",
    "* **OLTP:**\n",
    "  * Online Transaction Processing\n",
    "  * Real-Time,Transactional, row-based, frequent small writes/reads, save original and standard data \n",
    "  * Example: Banking, e-commerce\n",
    "\n",
    "* **OLAP:**\n",
    "  * Online Analytical Processing \n",
    "  * Analytical, columnar, large aggregations through ETL based on OLTP\n",
    "  * Example: Business intelligence, reporting\n",
    "\n",
    "* **Design goal:** \n",
    "  * OLTP → fast transactional integrity; \n",
    "  * OLAP → fast analytical queries\n",
    "\n",
    "OLTP： \n",
    "- 用途：处理日常事务型操作，强调 高并发、快速写入/更新/查询单条记录\n",
    "- 特点：实时性强，表结构通常规范化\n",
    "- 场景：银行转账、购物下单、库存更新\n",
    "\n",
    "OLAP：\n",
    "- 用途：用于分析和决策支持，强调 复杂查询、聚合、统计\n",
    "- 特点：查询慢但复杂，表结构通常星型/雪花型，存历史数据\n",
    "- 场景：销售数据分析、用户行为统计、财务报表\n",
    "\n",
    "---\n",
    "\n",
    "## **ETL vs ELT**\n",
    "\n",
    "* **ETL:** Extract → Transform → Load\n",
    "  * Transform before loading; good for traditional warehouses\n",
    "* **ELT:** Extract → Load → Transform\n",
    "  * Transform in data warehouse (e.g., Snowflake, Redshift); suits cloud-scale\n",
    "\n",
    "* **Reason ELT is preferred in cloud:** Scalability, compute separation, easier schema changes\n",
    "\n",
    "- ETL (Extract-Transform-Load)\n",
    "流程：先 抽取(Extract) → 转换(Transform) → 加载(Load)\n",
    "特点：数据在进入目标仓库前就已经清洗、转换好\n",
    "适用场景：传统数据仓库，目标库计算能力有限\n",
    "- ELT (Extract-Load-Transform)\n",
    "流程：先 抽取(Extract) → 加载(Load) → 在目标库转换(Transform)\n",
    "特点：依赖目标库强大的计算能力（如云仓库、分布式系统）进行转换\n",
    "适用场景：现代数据湖/云仓库，大数据分析场景\n",
    "\n",
    "| 特性    | ETL           | ELT           |\n",
    "| ----- | ------------- | ------------- |\n",
    "| 转换位置  | 在源或中间处理       | 在目标仓库处理       |\n",
    "| 适合数据量 | 小到中等          | 大数据量          |\n",
    "| 依赖    | 中间服务器或 ETL 工具 | 目标数据库/仓库计算能力强 |\n",
    "| 数据延迟  | 高，转换耗时        | 低，可直接加载原始数据   |\n",
    "\n",
    "ELT Code Sampe\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Extract & Load\n",
    "data = pd.read_csv('orders.csv')  # 抽取\n",
    "conn = sqlite3.connect('olap.db')\n",
    "data.to_sql('orders_raw', conn, if_exists='replace', index=False)  # 直接加载\n",
    "\n",
    "# Transform（在数据库里执行）\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "UPDATE orders_raw\n",
    "SET total_price = quantity * price\n",
    "WHERE total_price IS NULL\n",
    "''')\n",
    "conn.commit()\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **MapReduce execution flow**\n",
    "\n",
    "* **Map:** Process input splits, produce (key, value) pairs\n",
    "* **Shuffle/Sort:** Group values by key, redistribute across nodes\n",
    "* **Reduce:** Aggregate grouped values, write output\n",
    "\n",
    "### 1. 概念总结\n",
    "\n",
    "* **MapReduce** 是大数据分布式计算框架的核心编程模型\n",
    "* **目标**：处理大规模数据，通过 **Map（映射）** 和 **Reduce（归约）** 两步完成\n",
    "* **特点**：可扩展到成千上万节点，自动处理数据分片和容错\n",
    "\n",
    "### 2. 执行流程分解\n",
    "\n",
    "#### **1：Input Splitting（输入分片）**\n",
    "\n",
    "* 将大文件切成多个 **Input Split**\n",
    "* 每个 split 分配给一个 **Map Task**\n",
    "\n",
    "#### **2：Mapping（映射）**\n",
    "\n",
    "* Map 函数处理每个 split\n",
    "* **输入**：`<key, value>` 对\n",
    "* **输出**：中间 `<key, value>` 对\n",
    "\n",
    "#### **3：Shuffling & Sorting（洗牌和排序）**\n",
    "\n",
    "* 系统自动将 Map 输出根据 key 分组\n",
    "* 相同 key 的数据会发送到同一个 Reduce Task\n",
    "\n",
    "#### **4：Reducing（归约）**\n",
    "\n",
    "* Reduce 函数对相同 key 的所有 values 进行处理\n",
    "* 输出最终结果 `<key, aggregated_value>`\n",
    "\n",
    "#### **5：Output（输出结果）**\n",
    "\n",
    "* Reduce 输出写入 HDFS 或目标存储\n",
    "\n",
    "### 3. 图示流程（文字版）\n",
    "\n",
    "```\n",
    "[Input File]\n",
    "      │\n",
    "      ▼\n",
    "[Input Split] → 分配给多个 Map Task\n",
    "      │\n",
    "      ▼\n",
    "[Map Function] → 生成中间 <key, value>\n",
    "      │\n",
    "      ▼\n",
    "[Shuffle & Sort] → 相同 key 聚合\n",
    "      │\n",
    "      ▼\n",
    "[Reduce Function] → 汇总处理\n",
    "      │\n",
    "      ▼\n",
    "[Output File]\n",
    "```\n",
    "\n",
    "### 4. Python 模拟示例（Word Count）\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "\n",
    "#   1 & 2: Map\n",
    "data = [\"hello world\", \"hello mapreduce\"]\n",
    "intermediate = []\n",
    "for line in data:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        intermediate.append((word, 1))  # Map 输出 <word, 1>\n",
    "\n",
    "# Step 3: Shuffle & Sort\n",
    "shuffle_sort = defaultdict(list)\n",
    "for key, value in intermediate:\n",
    "    shuffle_sort[key].append(value)\n",
    "\n",
    "# Step 4: Reduce\n",
    "result = {}\n",
    "for key, values in shuffle_sort.items():\n",
    "    result[key] = sum(values)  # Reduce 聚合\n",
    "\n",
    "print(result)  # 输出: {'hello': 2, 'world': 1, 'mapreduce': 1}\n",
    "```\n",
    "\n",
    "> 核心：Map 负责拆和标记，Shuffle 按 key 聚合，Reduce 汇总计算。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### **Data Lake vs Data Warehouse**\n",
    "\n",
    "* **Data Lake:** Raw, unstructured/semi-structured data, low cost, flexible\n",
    "\n",
    "* **Data Warehouse:** Structured, cleaned, optimized for queries, high performance\n",
    "\n",
    "* **Use case:** \n",
    "  * Lakes → big raw data storage; \n",
    "  * Warehouse → analytics & reporting\n",
    "\n",
    "Data Lake-> Result after Loading of ELT\n",
    "- Data Lake = 集中存储各种原始数据，不要求立即清洗、转换或结构化\n",
    "- 目的是保留 原始、完整、可追溯的数据，以备后续分析或 ETL/ELT 使用\n",
    "- 数据格式可以是：CSV、JSON、Parquet、日志、图片、音视频等\n",
    "\n",
    "Data Warehouse -> Result after Extraction of ETL \n",
    "\n",
    "---\n",
    "\n",
    "###### **Batch Processing vs Streaming**\n",
    "\n",
    "* **Batch:** Process a large size of data at once in chunks\n",
    "  * Process Mode: Collect Data - Compute in Buiks - Output \n",
    "  * Characteristic\n",
    "    * High Latency ( minutes to hours or even days)\n",
    "    * High fault tolerance, suitable for large-scale computation \n",
    "    * Can recompute the batch, suitable for complex aggregations \n",
    "  * Use Case: Daily log analysis, reporting, ETL \n",
    "* **Streaming:** Process live data continuously as it arrives as small micro-batches \n",
    "  * Process Mode: Data arrives -> process immediately -> output \n",
    "  * Characteristic\n",
    "    * Low Latency(seconds)\n",
    "    * Real-time, Continuous Processing \n",
    "    * Requirement to high throughput and state management \n",
    "  * Use Case: Real-time clickstream analysis, monitoring, live logging \n",
    "\n",
    "* **Real-world use:** Batch → daily report, Streaming → live carpool tracking\n",
    "\n",
    "| 特性   | Batch Processing          | Streaming Processing                  |\n",
    "| ---- | ------------------------- | ------------------------------------- |\n",
    "| 数据输入 | 大块历史数据                    | 实时流式数据                                |\n",
    "| 延迟   | 高（分钟-小时）                  | 低（毫秒-秒）                               |\n",
    "| 容错   | 重算批次即可                    | 状态管理复杂，需 checkpoint                   |\n",
    "| 使用场景 | 报表、历史分析、ETL               | 实时监控、推荐、告警                            |\n",
    "| 计算粒度 | 批次                        | 单条或微批                                 |\n",
    "| 技术栈  | Hadoop, Spark Batch, Hive | Spark Streaming, Flink, Kafka Streams |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###### **Star Schema vs Snowflake Schema**\n",
    "\n",
    "* **Star Schema:** Fact table at center, denormalized dimension tables\n",
    "\n",
    "  * Fast for queries, simpler BI reports\n",
    "\n",
    "* **Snowflake Schema:** Dimension tables normalized\n",
    "\n",
    "  * Saves space, but slower joins\n",
    "\n",
    "* **BI preference:** Star schema → easier aggregations and faster query performance\n",
    "\n",
    "| Schema               | Description                                                                                            |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------------ |\n",
    "| **Star Schema**      | A central fact table connected directly to denormalized dimension tables. Simple, flat design.         |\n",
    "| **Snowflake Schema** | A fact table connected to normalized dimension tables, which may be further split into sub-dimensions. |\n",
    "\n",
    "| Feature             | Star Schema                      | Snowflake Schema                               |\n",
    "| ------------------- | -------------------------------- | ---------------------------------------------- |\n",
    "| Dimension Tables    | Denormalized                     | Normalized (may have sub-dimensions)           |\n",
    "| Complexity          | Simple, easy to understand       | More complex, hierarchical                     |\n",
    "| Query Performance   | Faster (fewer joins)             | Slower (more joins required)                   |\n",
    "| Storage Requirement | Higher (data redundancy)         | Lower (less redundancy)                        |\n",
    "| Maintenance         | Easier                           | Harder (updates in multiple tables)            |\n",
    "| Example Use Case    | OLAP dashboards, quick reporting | Data warehouses requiring strict normalization |\n",
    "\n",
    "Star Schema \n",
    "```\n",
    "       维度表1\n",
    "          │\n",
    "维度表2 ── 事实表 ── 维度表3\n",
    "          │\n",
    "       维度表4\n",
    "```\n",
    "\n",
    "Snowflake Schema \n",
    "```\n",
    "         子维表1\n",
    "            │\n",
    "         维度表1\n",
    "            │\n",
    "事实表 ── 维度表2 ── 子维表2\n",
    "```\n",
    "雪花模式将维度表规范化拆分，星型模式维度表扁平化存储。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fb643",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# 1. percentage of rides when it's carpool\n",
    "select 100*sum(case when ride_type == 'carpool' then 1 else 0)/count(*) from fct_ride;\n",
    "\n",
    "# 2. percentage of vehicle whose rides more carpool\n",
    "# 1 vehicle : n rides \n",
    "with rides_total as (\n",
    "    select vehicle_id,\n",
    "            sum(case when ride_type == 'carpool' then 1 else 0) as carpool,\n",
    "            sum(case when ride_type == 'regular' then 1 else 0) as regular\n",
    "    from fct_ride\n",
    "    group by vehicle_id \n",
    ")\n",
    "select 100*(sum(case when carpool>regular then 1 else 0))/count(*) from rides_total\n",
    "\n",
    "\n",
    "# 3. highest vehicle usage time \n",
    "select vehicle_id, sum(TIMESTAMP(SECOND, start_time, end_time)) as totals\n",
    "from fct_ride\n",
    "group by vehicle_id \n",
    "order by totals desc \n",
    "limit 1 "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
