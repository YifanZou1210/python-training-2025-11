# Sessions 1-3 
## What is Python's main characteristic regarding syntax compared to other programming languages?
  * Python use indentation instead of curly brace `{}` as block boundary
  * Python is interpreted language that is first compiled to bytecode, then executed by VM or interpreter
    * After typing `python3 script`, procedures like: source code(.py) -compiler-> bytecode(.pyc) -Execution-> Machine Instruction(runtime)
## What are the basic data types available in Python?
  * `int, float, str, list, tuple, dic, set, bool, None`
## Why is indentation important in Python?
  * It's symbolized code structure, begining and ending of a code block, espescially for some nested blocks, like for-loop, closure, etc
## What happens when you try to mix incompatible data types in an operation?
**Summary:**
* Python raises a `TypeError` when incompatible data types are mixed.
* Only operations explicitly defined for those types are allowed.
* Explicit type conversion is required to make operations valid.
* Most production errors come from unvalidated external input.

> Numeric types are only limited implicit coercion, Python does allow implicit widening within numeric types, like from int to float, bool to int ( must be safe numeric promotion )

**Details**
* Python is **strongly typed**, so it does not automatically convert unrelated data types.
* When incompatible types are used in an operation, Python raises a **TypeError** at runtime.
* This happens because the operation is not defined between those two types.
* Examples of incompatible operations:
  * Adding a string and an integer: `"5" + 3`
  * Comparing unrelated types: `"10" > 2`
  * Performing arithmetic on non-numeric objects
* Some mixed operations work only because Python explicitly defines them:
  * `"a" + "b"` â†’ string concatenation
  * `[1, 2] + [3]` â†’ list concatenation
* Explicit type conversion resolves most issues:
  * `int("5") + 3`
  * `str(3) + "5"`
* In backend systems, these errors commonly occur when handling request parameters, JSON payloads, or database results.
* Best practice is to validate and normalize data at system boundaries such as API layers.

## What is Git Flow?
**Summary**
Git Flow is a structured Git branching strategy that separates production code from development work. It defines clear rules for how features, releases, and hotfixes move through the system. This model improves stability and coordination for teams working on scheduled releases, but it can be heavy for fast-iteration environments.

###### Detailed answer

* Git Flow is a **branching model** that standardizes how teams create, merge, and release code.
* It uses two **long-lived branches**:

  * `main` (or `master`) holds production-ready code only.
  * `develop` holds the latest integrated development changes.
* **Feature branches** (`feature/*`) are created from `develop` and used to build new functionality in isolation.
* **Release branches** (`release/*`) are created from `develop` when a version is ready for final testing and stabilization.
* **Hotfix branches** (`hotfix/*`) are created from `main` to quickly fix production issues.
* After completion:

  * Features merge back into `develop`.
  * Releases merge into both `main` and `develop`.
  * Hotfixes merge into both `main` and `develop`.
* Git Flow works well for larger teams and products with planned release cycles.
* It is less suitable for continuous deployment or trunk-based development due to extra branching overhead.


## Explain the difference between `==` and `is` operators
  * `==` symbolizes liberal variable comparison 
  * `is` symbolizes reference address comparison in memory
## What's the difference between implicit and explicit type conversion?
  * former one used when one data type transfered to another one is safe and no data loss, like int to float
  * latter one used when user would like to use, like some conversion that implicit conversion fails, etc
## What's the difference between `if x:` and `if x == True:`?
  * former one checks if x condition is truth
  * latter one checks if x value equal to `True`, x type should be `boolean`


----


* What is the difference between mutable and immutable data types in Python?
  * Mutable data types can be changed after creation (e.g., `list`, `dict`, `set`), while immutable data types cannot be changed once created (e.g., `int`, `float`, `str`, `tuple`).

* What's the difference between a list and a tuple in Python?
  * Lists are mutable and can be modified (append, remove, etc.), while tuples are immutable and cannot be changed after creation. Tuples are generally faster and can be used as dictionary keys.

* What's the difference between `list.append()`, `list.extend()`, and `list.insert()`?
  *  `append(x)` adds a single element `x` at the end; `extend(iterable)` adds all elements from an iterable to the end; `insert(index, x)` inserts element `x` at the specified index, shifting other elements.

* Explain the difference between shallow copy and deep copy between `list.copy()`, `list[:]`, and `copy.deepcopy()`
  *  `list.copy()` and `list[:]` create a **shallow copy**, meaning the outer list is copied but inner mutable objects are shared. `copy.deepcopy()` creates a **deep copy**, recursively copying all nested objects.

* What are the advantages and disadvantages of using set comprehensions vs converting a list comprehension to a set?
  *  Set comprehensions `{x for x in iterable}` directly create a set and remove duplicates efficiently, whereas `set([x for x in iterable])` first creates a list then converts it to a set (slightly less efficient). Both achieve similar results, but direct set comprehension is cleaner.

* What's the time complexity difference between checking membership (`in` operator) in a list vs a set?
  *  Membership check in a list is O(n) (linear search), while in a set it is O(1) on average (hash table lookup).

* Why are tuples immutable but you can still modify a list inside a tuple?
  *  Tuples are immutable regarding the tuple object itself (cannot add/remove/replace elements), but if a tuple contains mutable objects like lists, those objects can be modified because the tuple only holds references to them.

* What will `my_list[::2]`, `my_list[::-1]`, and `my_list[1::3]` return for `my_list = [0,1,2,3,4,5,6,7,8,9]`?
    * `my_list[::2]` â†’ `[0, 2, 4, 6, 8]` (every 2nd element starting from index 0)
    * `my_list[::-1]` â†’ `[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]` (reversed list)
    * `my_list[1::3]` â†’ `[1, 4, 7]` (every 3rd element starting from index 1)

* What's the difference between `remove()`, `pop()`, and `del` for lists?
  * `remove(x)` deletes the first occurrence of value `x`; `pop(index)` removes and returns the element at `index` (default is last element); `del list[index]` deletes the element at `index` but does not return it. You can also use `del list[start:end]` to remove a slice.

----

* What is a lambda function, and how is it different from a regular function in Python?
  A lambda function is an anonymous, inline function defined with `lambda` keyword. It can take any number of arguments but has only one expression. Unlike regular functions defined with `def`, it does not have a name and cannot contain multiple statements.

* What is the difference between `*args` and `**kwargs` in function definitions?
  `*args` collects extra positional arguments as a tuple, while `**kwargs` collects extra keyword arguments as a dictionary.

* What is LEGB? Explain LEGB rule with a code example
  LEGB stands for Local, Enclosing, Global, Built-in, the order Python searches for variables. Example:

```python
x = "global"
def outer():
    x = "enclosing"
    def inner():
        x = "local"
        print(x)  # prints "local"
    inner()
outer()
```

* What is a closure in Python? How is it different from a regular nested function?
  A closure is a nested function that remembers values from its enclosing scope even after the outer function has finished execution. Regular nested functions do not retain these values once the outer function ends.

* What is the purpose of `if __name__ == "__main__":`?
  It ensures that a Python file runs certain code only when executed directly, not when imported as a module.

* Can you modify a global variable inside a function without using the `global` keyword?
  No, to reassign a global variable inside a function, you must use `global`. You can, however, mutate mutable objects like lists or dictionaries without `global`.

* In what order must you define parameters in a function signature?
  Positional arguments â†’ default arguments â†’ `*args` â†’ keyword-only arguments â†’ `**kwargs`.

* What is the difference between the `global` and `nonlocal` keywords?
  `global` allows modifying variables at the module level. `nonlocal` allows modifying variables in the nearest enclosing non-global scope.

* What is a common pitfall when using mutable default arguments?
  Using mutable defaults (like lists or dictionaries) can lead to unexpected shared state across function calls.

* What is a higher-order function? Give examples of built-in higher-order functions
  A higher-order function takes functions as arguments or returns a function. Examples: `map()`, `filter()`, `reduce()`, `sorted()` with key function.

---
# session-4-oop

## 1. What are the four principles of OOP?
* **Encapsulation** â€“ Hides internal data; accessed through public methods or properties.
* **Abstraction** â€“ Exposes only essential behavior; hides complexity.
* **Inheritance** â€“ Allows classes to derive and extend functionality from a base class.
* **Polymorphism** â€“ Enables different classes to define methods with the same name but different behaviors.

**Example:**

```python
class Animal:
    def speak(self): pass

class Dog(Animal):
    def speak(self): return "Woof"

class Cat(Animal):
    def speak(self): return "Meow"
```


## 2. What's the difference between__str__ and __repr__ magic methods?

* `__str__`: For **readable** string representation (used by `print()` or `str()`).
* `__repr__`: For **developer/debug** representation (used by `repr()` or console).

**Example:**

```python
class Book:
    def __str__(self):
        return "Readable: Book Info"
    def __repr__(self):
        return "Book(title='X', author='Y')"
```


## 3. How do magic methods like __eq__ affect object comparison?

* `__eq__` defines how objects are compared using `==`.
* Without it, equality compares **object identity (memory address)** by default.

**Example:**

```python
class Point:
    def __init__(self, x, y):
        self.x, self.y = x, y
    def __eq__(self, other):
        return self.x == other.x and self.y == other.y
```


## 4. Explain the difference between `@classmethod` and `@staticmethod`.

* `@classmethod`: Receives the **class itself (`cls`)** as the first argument; can modify class state.
  * ç±»æ–¹æ³•ï¼šä»¥ç±»æœ¬èº«clsä¸ºç¬¬ä¸€ä¸ªå‚æ•°
  * å¯ä»¥è®¿é—®/ä¿®æ”¹ç±»å˜é‡
  * å¸¸ç”¨äºFactory Method, æ„é€ é€»è¾‘
* `@staticmethod`: Receives **no automatic first argument**; behaves like a regular function in a class.
  * é™æ€æ–¹æ³•ä¸ä¾èµ–å®ä¾‹self,ä¹Ÿä¸ä¾èµ–ç±»cls
  * æœ¬è´¨åªæ˜¯æ”¾åœ¨ç±»å‘½åç©ºé—´é‡Œçš„æ™®é€šå‡½æ•°
  * ä¸èƒ½è®¿é—®å®ä¾‹å˜é‡ã€ç±»å˜é‡

**Example:**

```python
class Example:
    count = 0

    @classmethod
    def set_count(cls, value):
        cls.count = value

    @staticmethod
    def add(x, y):
        return x + y
```


## 5. What are property decorators in Python?

 
Used to define **getter/setter/deleter** for attributes â€” enabling **controlled access** like Java getters/setters.
æŠŠæ–¹æ³•ä¼ªè£…æˆå±æ€§ï¼Œå¯¹å¤–æ˜¯è®¿é—®å­—æ®µï¼Œå¯¹å†…æ‰§è¡Œæ–¹æ³•é€»è¾‘(getter, setter, deleter), é¿å…public field ç›´æ¥æš´éœ²å†…éƒ¨æ–¹æ³•ç»†èŠ‚ä¸å®‰å…¨ï¼Œä¿æŒäº†apiç¨³å®š

**Example:**

```python
class Person:
    def __init__(self, age):
        self._age = age

    @property
    def age(self):
        return self._age

    @age.setter
    def age(self, value):
        if value < 0:
            raise ValueError("Invalid age")
        self._age = value
```


## 6. What's the difference between public, protected (`_`), and private (`__`) attributes?

 

| Type      | Prefix | Access Level                               | Example         |
| --------- | ------ | ------------------------------------------ | --------------- |
| Public    | none   | Accessible anywhere                        | `self.name`     |
| Protected | `_`    | Conventionally internal (not enforced)     | `self._balance` |
| Private   | `__`   | Name-mangled; accessible only within class | `self.__secret` |


## 7. What's Singleton pattern? How to implement it?

 
Ensures **only one instance** of a class exists.

**Example:**

```python
class Singleton:
    _instance = None
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
```


## 8. What's Factory pattern? How to implement it?

 
Provides a **method to create objects** without specifying exact class names.

**Example:**

```python
class ShapeFactory:
    def get_shape(self, shape_type):
        if shape_type == "circle": return Circle()
        if shape_type == "square": return Square()
```


## 9. What is the `self` parameter?

 
Represents the **current instance** of a class; used to access instance variables and methods.

**Example:**

```python
class Car:
    def __init__(self, brand):
        self.brand = brand
```


## 10. What are abstract base classes (ABC) in Python?

 
Define **interfaces** that subclasses must implement, using `abc` module.

**Example:**

```python
from abc import ABC, abstractmethod

class Animal(ABC):
    @abstractmethod
    def speak(self):
        pass
```

---

# session-5-advanced concept of python 

## **What is a decorator in Python, and where is it used?**
  A callable that wraps another function or method to modify or enhance its behavior without changing its code; commonly used for logging, authentication, caching, or timing.
## **What's the difference between a generator and a regular function that returns a list?**
  A generator yields items one at a time lazily, consuming less memory; a regular function builds and returns the entire list at once, consuming memory for all elements immediately.
## **When would you choose generators over lists, and what are the memory implications?**
  Use generators for large or infinite sequences to avoid storing all elements in memory; memory usage is minimal since only the current element is kept.
## **Explain the difference between threading, multiprocessing, and asyncio in Python**
  Threading: multiple threads share memory, good for IO-bound; Multiprocessing: separate processes, each with own memory, bypasses GIL, good for CPU-bound; Asyncio: single-threaded cooperative concurrency, efficient for high-latency IO tasks.
## ğŸ”´ **What is the Global Interpreter Lock (GIL)? How does it affect threading and multiprocessing?**
  GIL allows only one Python bytecode instruction to execute at a time per process; threading is limited for CPU-bound tasks, multiprocessing bypasses GIL and can fully use multiple cores.

GIL is Global Lock in Interpreter level, which exists coercively and transparently. Unique feature in Python concurrency model, Java doesn't have. 
## **When to use threading, asyncio, multiprocess?**
  Threading: IO-bound tasks; Asyncio: high-concurrency IO tasks; Multiprocessing: CPU-bound tasks needing parallel execution.
## **What are CPU-bound vs IO-bound tasks?**
  CPU-bound: tasks limited by computation speed; IO-bound: tasks limited by input/output latency, e.g., network, disk.
## **What's the difference between yield and return in a function**
  `return` sends a final value and exits the function; `yield` produces a value and pauses function, allowing iteration to resume later.
## **What's the difference between using open() with explicit close() vs using the with statement**
  Explicit close() requires manual cleanup; `with` ensures automatic resource release even if exceptions occur.
## **How to handle exceptions? Why is exception handling important?**
  Use try-except blocks to catch and respond to errors; important to prevent crashes, maintain program flow, and provide meaningful error messages.

---

# session-6-db

## What are primary keys and foreign keys? How are they used in relational databases?
  - A primary key is a column or set of columns in a table that uniquely identifies each row. A foreign key is a column or set of columns in one table that refers to the primary key in another table, establishing a relationship between the two tables. They are used to enforce data integrity and relational links between tables.

## What is the difference between INNER JOIN, LEFT JOIN, and FULL OUTER JOIN
  - INNER JOIN returns only the rows that have matching values in both tables. LEFT JOIN returns all rows from the left table and the matched rows from the right table, with NULLs for unmatched rows. FULL OUTER JOIN returns all rows when there is a match in either table, filling in NULLs for missing matches.

## What is normalization? æ•°æ®åº“èŒƒå¼
  - Normalization is the process of organizing database tables and columns to reduce redundancy and improve data integrity, typically through a series of normal forms (1NF first normal form, 2NF, 3NF, etc.).

## What are the different types of database relationships (1:1, 1:many, many:many) and how do you implement them in SQL?
  - 1:1 relationship: each row in Table A is linked to exactly one row in Table B. Implemented by having a foreign key in either table referencing the otherâ€™s primary key with UNIQUE constraint.
  - 1:many relationship: each row in Table A can be linked to multiple rows in Table B. Implemented by having a foreign key in the â€œmanyâ€ table referencing the primary key of the â€œoneâ€ table.
  - Many:many relationship: rows in Table A can be linked to multiple rows in Table B and vice versa. Implemented using a junction table with foreign keys referencing both tables.

## ğŸ”´ What are transactions and isolation levels? Explain the problems each isolation level solves.
  - Transactions are sequences of database operations executed as a single unit. Isolation levels control visibility of changes between concurrent transactions.

  * Read Uncommitted: allows dirty reads; solves no problem, lowest isolation.
  * Read Committed: prevents dirty reads; solves the dirty read problem.
  * Repeatable Read: prevents dirty and non-repeatable reads; solves dirty and non-repeatable read problems.
  * Serializable: prevents dirty, non-repeatable, and phantom reads; ensures full isolation, highest level.

Read Uncommittedï¼ˆæœªæäº¤è¯»ï¼‰
- ç‰¹ç‚¹ï¼šäº‹åŠ¡å¯ä»¥è¯»å–å…¶ä»–äº‹åŠ¡å°šæœªæäº¤çš„æ•°æ®ã€‚
- å¯èƒ½é—®é¢˜ï¼šè„è¯»ï¼ˆDirty Readï¼‰
- ç¤ºä¾‹ï¼š
  - äº‹åŠ¡ A æ”¹äº†æŸæ¡æ•°æ®ï¼Œä½†è¿˜æ²¡æäº¤
  - äº‹åŠ¡ B è¯»åˆ°äº†è¿™ä¸ªæœªæäº¤çš„ä¿®æ”¹
  - å¦‚æœäº‹åŠ¡ A å›æ»šï¼Œäº‹åŠ¡ B è¯»çš„æ•°æ®å°±æ˜¯é”™è¯¯çš„

Read Committedï¼ˆå·²æäº¤è¯»ï¼‰
- ç‰¹ç‚¹ï¼šåªèƒ½è¯»å–å…¶ä»–äº‹åŠ¡å·²æäº¤çš„æ•°æ®
è§£å†³è„è¯»ï¼Œä½†ä»å¯èƒ½å‡ºç°ï¼š
ä¸å¯é‡å¤è¯»ï¼ˆNon-Repeatable Readï¼‰: different reads in same transaction
- ç¤ºä¾‹ï¼š
  - äº‹åŠ¡ B åœ¨ä¸¤æ¬¡æŸ¥è¯¢ä¸­ï¼Œç¬¬ä¸€æ¬¡çœ‹åˆ°å€¼ X
  - äº‹åŠ¡ A åœ¨ä¸­é—´æäº¤äº†ä¿®æ”¹
  - ç¬¬äºŒæ¬¡æŸ¥è¯¢çœ‹åˆ°å€¼ Y â†’ ä¸å¯é‡å¤è¯»

Repeatable Readï¼ˆå¯é‡å¤è¯»ï¼‰
- ç‰¹ç‚¹ï¼šåœ¨ä¸€ä¸ªäº‹åŠ¡å†…ï¼Œå¤šæ¬¡è¯»å–åŒä¸€æ¡æ•°æ®çš„ç»“æœä¸€è‡´
- è§£å†³è„è¯»å’Œä¸å¯é‡å¤è¯»ï¼Œä½†ä»å¯èƒ½å‡ºç°ï¼š
å¹»è¯»ï¼ˆPhantom Readï¼‰
- ç¤ºä¾‹ï¼š
  - äº‹åŠ¡ B æŸ¥è¯¢æŸæ¡ä»¶ä¸‹çš„å¤šè¡Œè®°å½•
  - äº‹åŠ¡ A æ’å…¥ä¸€æ¡æ»¡è¶³æ¡ä»¶çš„æ–°è®°å½•å¹¶æäº¤
  - äº‹åŠ¡ B å†æ¬¡æŸ¥è¯¢æ—¶ä¼šçœ‹åˆ°æ–°è®°å½• â†’ å¹»è¯»

Serializableï¼ˆå¯ä¸²è¡ŒåŒ–ï¼‰
- ç‰¹ç‚¹ï¼šæœ€é«˜éš”ç¦»çº§åˆ«ï¼Œäº‹åŠ¡åƒä¸²è¡Œæ‰§è¡Œä¸€æ ·
è§£å†³è„è¯»ã€ä¸å¯é‡å¤è¯»ã€å¹»è¯»
- ç¼ºç‚¹ï¼šå¹¶å‘æ€§èƒ½ä½ï¼Œå®¹æ˜“é€ æˆé”ç­‰å¾…æˆ–æ­»é”

## What's the difference between PRIMARY KEY, UNIQUE, and FOREIGN KEY constraints?
  - PRIMARY KEY enforces unique and non-null values for the column(s). UNIQUE enforces uniqueness but allows one NULL per column (depending on DBMS). FOREIGN KEY enforces referential integrity by linking to a primary key in another table.

---
# session-7-database-advanced 

## 1. è§†å›¾ã€ç‰©åŒ–è§†å›¾ã€è¡¨çš„åŒºåˆ«

What's difference between View, Materialized View, and Table

* **è¡¨ï¼ˆTableï¼‰**ï¼šçœŸå®å­˜å‚¨æ•°æ®ï¼Œå¢åˆ æŸ¥æ”¹éƒ½æ˜¯ç›´æ¥ä¿®æ”¹è¡¨ä¸­æ•°æ®
* **è§†å›¾ï¼ˆViewï¼‰**ï¼šä¸å­˜æ•°æ®ï¼Œåªå­˜å‚¨sqlå®šä¹‰ï¼Œ æŸ¥è¯¢è§†å›¾æ—¶ä¼šå®æ—¶æŸ¥è¯¢æ‰§è¡Œä»–èƒŒåçš„sql
* **ç‰©åŒ–è§†å›¾ï¼ˆMaterialized Viewï¼‰**ï¼šå­˜å‚¨sqlæŸ¥è¯¢ç»“æœï¼Œç±»ä¼¼ç¼“å­˜è¡¨ï¼ŒæŸ¥è¯¢é€Ÿåº¦å¿«ï¼Œå› ä¸ºç»“æœæ˜¯premakeçš„ï¼Œ ä½†æ˜¯æ•°æ®å¯èƒ½è¿‡æ—¶ï¼Œéœ€è¦åˆ·æ–°ï¼Œå¸¸ç”¨äºåˆ†æã€æŠ¥è¡¨ã€å¤æ‚èšç±»æŸ¥è¯¢

- Table is physically stored data on disk, which implementation data changes through insertion, deletion, addition, etc, the primary storage structure in a database
- View does not store data, only store sql definition, each time we query a view, the database will rerun sql on underlying tables
- Materialized View store actual query res physically, like a cached table, it query very fast cuz res are precomputed, but data can be stale, needs to refresh frequently

## 2. ä»€ä¹ˆæ˜¯ ORMï¼Ÿä¸ºä»€ä¹ˆéœ€è¦ ORMï¼Ÿ

What is ORM? Why do we need ORM?

ORM å°†æ•°æ®åº“è¡¨æ˜ å°„ä¸ºç¼–ç¨‹è¯­è¨€å¯¹è±¡ï¼Œè®©å¼€å‘è€…ç”¨å¯¹è±¡æ“ä½œæ•°æ®è€Œä¸å†™ SQLã€‚
ä¼˜ç‚¹ï¼šå‡å°‘ SQLã€é¿å…æ³¨å…¥ã€ç®¡ç†äº‹åŠ¡ä¸å…³ç³»ã€è·¨æ•°æ®åº“è¿ç§»å®¹æ˜“ã€‚

ORM stands for Object-Relational-Mapping, a concept but not specific tool, a technique that maps object-oriented models to relational database tables, so developers can work with objects instead of writing SQL mannully. 

ORM supports:
- Automatic SQL generation 
- Object lifecyle management 
- Caching 
- Transaction

#### ORM vs JPA vs Hibernate 
- ORM: a general concept or techinique 
- JPA: stands for Java Persistence API. Java standard specification for ORM 
  - not a framework, not provide implementation, and defines interfaces, annotations, behavior rules 
  - support abstract interface of core APIs(`EntityManager`, `Query`, etc), annotations(`@Entity, @Id, @OneToMany`), and manage entity lifecycle and persistence rules 
- Hibernate: a concrete ORM framework and the most common JPA implementation 
  - offers JPA interface implementations, Advanced caching, HQL, Performance tuning options, Batch processing optimizations 



## 3. ACID å±æ€§è§£é‡Š

Explain the ACID Properties

* **åŸå­æ€§ atomicity**ï¼šå…¨éƒ¨æˆåŠŸæˆ–å…¨éƒ¨å¤±è´¥
* **ä¸€è‡´æ€§ consistency**ï¼šæ•°æ®è§„åˆ™ä¸è¢«ç ´å
* **éš”ç¦»æ€§ isolation**ï¼šå¹¶å‘äº‹åŠ¡äº’ä¸å¹²æ‰°
* **æŒä¹…æ€§ durability**ï¼šæäº¤åæ•°æ®ä¸ä¼šä¸¢å¤±

ç¡®ä¿æ•°æ®åº“ä¸ä¼šå‡ºç°è„å†™ã€éƒ¨åˆ†æ›´æ–°æˆ–æ•°æ®ä¸¢å¤±ã€‚

ACID stands for:
- Atomicity: one transaction either fully succeed or fully fails to prevent partial updates 
- Consistency: one transaction moves db from one valid state to another one, respecting constraints to avoid invalid states
- Isolation: concurrent transations behave as if executed one by one to avoid diry read, non-repeatable read, phantom read 
- Durability: once transaction is commited, result should be saved even if system crushed through disk persistence or replication 


## 4. CAP å®šç†

Explain the CAP Theorem
- States that a distributed system can only guarantee two of three requirements: Consistency, Availability, Parition Tolerance 
- Consistency: Each Node return up-to-date write, no matter which node is queried, usual applied system like Bank balance, financial transactions 
- Availability: Every request receives a non-error response, even during replica's failure or isolated, usually used in Social feeds, Messaging Sys
- Parition Tolerance: The sys continues operating even if network failures between nodes 

Why P is mandatory 
In real distributed systems:
- Network failures are unavoidable
- Cloud, cross-region, multi-AZ setups always risk partitions


## 5. SQL ä¸ NoSQL çš„ä½¿ç”¨åœºæ™¯

When choose SQL vs NoSQL?

SQLï¼šç»“æ„åŒ–æ•°æ®ã€å¤æ‚æŸ¥è¯¢ã€äº‹åŠ¡è¦æ±‚å¼ºï¼ˆé“¶è¡Œã€è®¢å•ï¼‰ã€‚
NoSQLï¼šè¶…å¤§è§„æ¨¡ã€çµæ´»ç»“æ„ã€é«˜å¹¶å‘è¯»å†™ï¼ˆæ—¥å¿—ã€ç¤¾äº¤ã€ç¼“å­˜ï¼‰ã€‚

SQL fits systems with structured data, clear relationships, and strong transaction requirements. NoSQL shines when you need huge scale, flexible data shapes, or extremely high read/write throughput. It's more about choosing the right tool for the pattern of your data and traffic.

---

## 6. ä»€ä¹ˆæ˜¯æœ€ç»ˆä¸€è‡´æ€§

What is Eventual Consistency?

* Eventual consistency means updates might not show up instantly everywhere, but if nothing else changes, all nodes will eventually catch up and show the same data. It trades immediate correctness for better performance and availability.

---

## 7. åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸€è‡´æ€§æ¨¡å‹

What's difference between Consistency Models: Strong, Weak, Eventual

* **å¼ºä¸€è‡´æ€§**ï¼šæ¯æ¬¡è¯»å–éƒ½æ˜¯æœ€æ–°å€¼
* **å¼±ä¸€è‡´æ€§**ï¼šä¸ä¿è¯è¯»å–æœ€æ–°å€¼
* **æœ€ç»ˆä¸€è‡´æ€§**ï¼šçŸ­æœŸä¸ä¸€è‡´ï¼Œæœ€ç»ˆæ”¶æ•›

Strong consistency means every read sees the latest write. Weak consistency makes no such promise. Eventual consistency is in the middleâ€”you might read stale data now, but the system will synchronize over time. It's about choosing speed vs correctness.


## 8. æ¨ªå‘æ‰©å±• vs çºµå‘æ‰©å±•

What's difference between Horizontal vs Vertical Scaling

* **æ¨ªå‘æ‰©å±•**ï¼šåŠ æœºå™¨ï¼›åˆ†å¸ƒå¼æ‰©å®¹ï¼›å¯çº¿æ€§æ‰©å±•ã€‚
* **çºµå‘æ‰©å±•**ï¼šæå‡å•æœºæ€§èƒ½ï¼›å—ç¡¬ä»¶æé™é™åˆ¶ã€‚

* **Horizontal**: Add more servers
* **Vertical**: Add more CPU/RAM to one machine
Vertical scaling means giving a single machine more power. Horizontal scaling means adding more machines to share the load. Scaling out is more flexible long-term but requires more distributed-system thinking.

## 9. MongoDB å¦‚ä½•æ”¯æŒ ACID

### (How does MongoDB handle ACID?)

**ç­”æ¡ˆï¼ˆä¸­æ–‡ï¼‰**

* å•æ–‡æ¡£å¤©ç„¶ ACID
* å¤šæ–‡æ¡£äº‹åŠ¡ï¼ˆ4.0+ï¼‰æ”¯æŒå®Œæ•´ ACID
* åˆ†ç‰‡é›†ç¾¤ä»æ˜¯æœ€ç»ˆä¸€è‡´ï¼ˆå¤åˆ¶å»¶è¿Ÿï¼‰
* äº‹åŠ¡å¼€é”€å¤§ï¼Œä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨

**Answer (English)**

* Single-document ops are inherently ACID
* Multi-doc ACID transactions supported since 4.0
* Sharded clusters remain eventually consistent
* Transactions are expensive; use only when needed

MongoDB is fully ACID for single-document updates. Since version 4.0 it also supports multi-document transactions, making it behave much more like a relational DB when needed. But multi-document transactions cost performance, so MongoDB encourages embedding related data in one document.


## 10. ä»€ä¹ˆæ˜¯åˆ†ç‰‡ï¼Ÿå’Œåˆ†åŒºæœ‰ä½•åŒºåˆ«ï¼Ÿ

What is Sharding? How does it differ from Partitioning?

* **åˆ†ç‰‡**ï¼šæŠŠæ•°æ®æ‹†åˆ°å¤šå°æœåŠ¡å™¨ï¼ˆåˆ†å¸ƒå¼æ¨ªå‘æ‰©å±•ï¼‰
* **åˆ†åŒº**ï¼šæŠŠå•è¡¨æ‹†æˆå¤šä¸ªé€»è¾‘/ç‰©ç†åˆ†åŒºï¼ˆåŒä¸€æœåŠ¡å™¨å†…éƒ¨ï¼‰

**åŒºåˆ«ï¼š**åˆ†ç‰‡=è·¨æœºå™¨ï¼›åˆ†åŒº=å•æœºå†…éƒ¨ã€‚

* **Sharding**: Split data across multiple servers
* **Partitioning**: Split data within one server/table

Partitioning splits data inside one database instance. Sharding splits data across multiple servers, each holding part of the dataset. Sharding is basically distributed partitioning and helps scale out when one machine canâ€™t handle the load.

---
# session-8-http-flask
**How does routing work in Flask?**

Routing in Flask works by associating URLs with Python functions using decorators like `@app.route('/path')`. When a request matches the URL pattern, Flask calls the corresponding function to handle it.

## **What is restful service**

A RESTful service is a web service that follows REST principles: stateless communication, resource-based URLs, standard HTTP methods (GET, POST, PUT, DELETE), and representation in JSON or XML.

## **What are the categories of HTTP status codes (1xx, 2xx, 3xx, 4xx, 5xx)? Provide examples for each.**

* 1xx: Informational (e.g., 100 Continue)
* 2xx: Success (e.g., 200 OK, 201 Created)
* 3xx: Redirection (e.g., 301 Moved Permanently, 302 Found)
* 4xx: Client error (e.g., 400 Bad Request, 404 Not Found)
* 5xx: Server error (e.g., 500 Internal Server Error, 503 Service Unavailable)

## **What is HTTP and how does it work**

HTTP (Hypertext Transfer Protocol) is a request-response protocol where clients (browsers, apps) send requests to servers over TCP, and servers respond with status codes and content.

## **Explain the concept of idempotency in HTTP methods**

Idempotency means that performing the same HTTP request multiple times has the same effect as performing it once. For example, GET, PUT, and DELETE are idempotent; POST is generally not.
- GET: Natively idempotency
- PUT: Update target with request's content, target resource no change during multiple requests 
- DELETE: same with above 

## **Explain the difference between HTTP and HTTPS**

HTTP is unsecured and sends data in plaintext; HTTPS uses SSL/TLS to encrypt communication, ensuring safety and integrity.

## **Design a RESTful API for a blogging platform**

* `GET /posts` â†’ list all posts
* `GET /posts/<id>` â†’ get a single post
* `POST /posts` â†’ create a new post
* `PUT /posts/<id>` â†’ update a post
* `DELETE /posts/<id>` â†’ delete a post
* `GET /posts/<id>/comments` â†’ list comments for a post

## **What is the MVC architecture**

MVC architecture separates an application into:

* Model â†’ data and business logic
* View â†’ user interface
* Controller â†’ handles input and updates Model/View

## **What are Flask's request objects**

Flaskâ€™s request objects encapsulate HTTP request data, including:

* `request.args` â†’ query parameters
* `request.form` â†’ form data
* `request.json` â†’ JSON payload
* `request.headers` â†’ HTTP headers
* `request.method` â†’ HTTP method used


---
# session-9-authN-authZ

**Difference between authentication and authorization**
Authentication verifies who a user is (identity check), while authorization determines what an authenticated user is allowed to do (permissions/access control).

## **What are HTTP cookies? How do they work and what are their main use cases?**
HTTP cookies are small pieces of data stored by the browser and sent with requests to the server. They work by the server sending `Set-Cookie` headers, which the browser stores and includes in subsequent requests. Main use cases: session tracking, user preferences, authentication tokens.

## **What are the limitations of cookies**

* Limited size (~4KB per cookie)
* Sent with every HTTP request (can increase bandwidth)
* Security risks: susceptible to XSS, CSRF if not configured properly
* Domain and path restrictions can complicate usage

## **What is JWT and how does it work?**
JWT (JSON Web Token) is a compact, self-contained token that encodes claims in JSON, signed using a secret or public/private key. It works by the server issuing a token after authentication; clients include the token in requests for stateless verification without server-side session storage.

jwtæ˜¯ä¸€ç§ æ— çŠ¶æ€çš„ä»¤ç‰Œæ ¼å¼ï¼Œç”¨äº ç”¨æˆ·è®¤è¯ ä¸ æˆæƒã€‚æœåŠ¡ç«¯ä¸ä¿å­˜ sessionï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½ç¼–ç åœ¨ token å†…ï¼Œç”±å®¢æˆ·ç«¯æºå¸¦ã€‚
ç»„æˆï¼š header+payload+signature 
å·¥ä½œæµç¨‹ï¼š 
- ç”¨æˆ·ç™»å½•å¹¶è¾“å…¥æ­£ç¡®å‡­è¯ã€‚
- æœåŠ¡ç«¯éªŒè¯å¯†ç  â†’ ç”Ÿæˆ JWTï¼ˆå«ç”¨æˆ· IDã€è§’è‰²ç­‰ï¼‰ã€‚
- å®¢æˆ·ç«¯ä¿å­˜ tokenï¼ˆé€šå¸¸åœ¨ localStorage æˆ– HTTP-only Cookieï¼‰ã€‚
- å®¢æˆ·ç«¯åç»­è¯·æ±‚åœ¨ Authorization: Bearer <token> ä¸­å‘é€ JWTã€‚
- æœåŠ¡ç«¯æ”¶åˆ°è¯·æ±‚ï¼š
- éªŒè¯ç­¾å
- æ£€æŸ¥è¿‡æœŸæ—¶é—´
- è§£å‡ºç”¨æˆ·è§’è‰²/æƒé™
- å¦‚æœéªŒè¯æˆåŠŸ â†’ æ”¾è¡Œï¼Œå¦åˆ™è¿”å› 401ã€‚

## **What are the advantages and disadvantages of using JWT compared to traditional session-based authentication?**
Advantages:

* Stateless: no server-side session storage required
* Portable across domains and microservices
* Can include custom claims

Disadvantages:

* Token revocation is harder
* Token size larger than session ID
* Sensitive info must be encrypted if included

## **How do you invalidate or blacklist JWT tokens?**

* Maintain a server-side blacklist of token IDs until expiration
* Issue short-lived access tokens and use refresh tokens
* Change the signing key to invalidate all existing tokens

## **What is password hashing and why is it important?**
Password hashing converts a plaintext password into a fixed-length, irreversible string using algorithms like bcrypt or Argon2. It is important to protect passwords in case of database breaches and prevent storing plaintext passwords.

## **What is the access / refresh token pattern**

* **Access token**: short-lived token for accessing protected resources
* **Refresh token**: long-lived token used to obtain new access tokens without re-authentication
* Pattern improves security and reduces the need to store long-lived tokens on clients

## **What is the Oauth2 flow**
OAuth2 flow allows third-party apps to access resources on behalf of users without sharing passwords. Common flows:

* **Authorization Code**: app gets a code via user authorization, then exchanges it for tokens
* **Implicit**: token returned directly in URL fragment (mostly deprecated)
* **Resource Owner Password Credentials**: user provides credentials to app (legacy, less secure)
* **Client Credentials**: app accesses its own resources, no user involved

---

# session-10-fast api 
## 1. **Explain the difference between def and async def in FastAPI route handlers. When should you use each?**

   * `def` defines a synchronous function; it blocks the event loop while executing.
     * Synchronous `def` function blocks the event loop cuz it never yields control back(äº¤å‡ºæ§åˆ¶æƒ). The event loop runs on a single thread, without `await`, it cannot switch to other tasks. 
     * Event Loop is a single-threaded scheduler, only run one piece of code at a time and switch tasks at `await` time
     * Calling a synchronous function inside an async function still blocks 
   * `async def` defines an asynchronous coroutine; allows concurrent request handling when awaiting I/O.
   * **Use `async def`** for I/O-bound operations (DB queries, HTTP calls); use `def` for CPU-bound or synchronous code.

> AsyncIOæ˜¯Single-Thread Level Concurrency ModelåŸºäºevent loop, ä½¿ç”¨coroutine+await,ä¸»åŠ¨è®©å‡ºcontrol, é€‚åˆIO-bound, *åªæœ‰ä¸€ä¸ªthread*, æ²¡æœ‰concurrency

> Multithreadæ˜¯OS Levelçš„Concurrency Model, å¤šä¸ªçº¿ç¨‹ï¼ŒOSè°ƒåº¦ï¼ŒæŠ¢å å¼ï¼Œå¯concurrent(multi-cpu), 
## 2. **What is dependency injection in FastAPI and how does it work behind the scene?**

   * Dependencies are functions that provide values or setup/cleanup for route handlers.
   * Declared using `Depends()`.
   * **Behind the scenes:** FastAPI builds a dependency graph per request, resolves it recursively, and injects returned values into handler parameters.

**DI in FastAPI vs Springboot**

FastAPI:
- Function-Based DI, Dependencies are plain function, No global Container, easy to reason about request scope 
- Dependencies are declared adn scoped per request, no global lifecycle management, cleanup via `yield`
- Resolution happens at runtime 
- Explicit and lightweight 

Spring Boot 
- Contrainer-based DI 
- Dependencies are objects, managed as beans, auto-wired
- Resolution happens at startup 
- Framework-driven and implicit 


## 3. **How does FastAPI achieve automatic API documentation?**

   * Generates OpenAPI schema from route definitions, type hints, and Pydantic models.
   * Exposes Swagger UI (`/docs`) and ReDoc (`/redoc`) automatically.
   * Type hints + Pydantic models allow FastAPI to infer request/response structure and validation for docs.

## 4. **Explain the difference between Path, Query, Header, Body, and Cookie parameters in FastAPI.**

   * `Path`: part of URL path variables, required (like `/items/{id}`), define which resource u required. 
   * `Query`: query string parameters after question mark, placed in function parameter list, optional by default (`?q=test`), frequent usage are pagination, filters, search-keywords 
```python
# GET /users?page=2&limit=50
from typing import Optional 
@app.get("/users")
def list_users(page:int = 1, limit:int = 20):
  return {"page": page, "limit": limit}
```
   * `Header`: HTTP headers (`X-Token`).
   * `Body`: request body, usually JSON; often Pydantic models.
   * `Cookie`: HTTP cookies.
   * FastAPI uses parameter type and `Path()`, `Query()`, etc., to extract values from requests.

## 5. **What is the purpose of Pydantic models in FastAPI? How do they differ from SQLAlchemy/SQLModel database models?**

   * Pydantic: validate and serialize data, parse request/response JSON; not tied to DB.
   * SQLAlchemy/SQLModel: define database tables, ORM mapping, handle persistence.
   * Pydantic focuses on input/output structure; SQLAlchemy focuses on storage.

## 6. **Explain how FastAPI's dependency caching works within a single request. Why is this important?**

   * Dependencies are cached once per request; repeated `Depends()` calls with the same function return the same object.
   * Ensures efficiency and consistency, e.g., DB session reused across multiple parameters/routes in a request.

## ğŸ”´ 7. **How does FastAPI handle request validation and what happens when validation fails? How can you customize error responses?**

   * Validates requests automatically via Pydantic and type hints.
   * On failure, returns `422 Unprocessable Entity` with details.
   * Customize errors using `@app.exception_handler(RequestValidationError)`.

## 8. **Explain the difference between using Annotated[Session, Depends(get_db)] vs Session = Depends(get_db) for type hints. Which is recommended and why?**

   * `Annotated[Session, Depends(get_db)]`: separates type hint from dependency injection; preferred for clarity and type checking.
   * `Session = Depends(get_db)`: mixes type hint and dependency.
   * **Recommendation:** use `Annotated` for clearer typing and better IDE/type-checker support.

---
# session-11-fast api

## 1. **What's the difference between using `requests` and `httpx` for making HTTP calls in FastAPI, and why is `httpx` preferred in async contexts?**

   - `requests` is a synchronous HTTP client; every request blocks the thread until the response arrives, not suitable for concurrency  
   - `httpx` supports both synchronous and asynchronous requests, allowing `async`/`await` syntax, supporting NIO HTTP requests
   - In async FastAPI routes, `httpx` is preferred because it allows non-blocking calls, letting other coroutines run concurrently and improving performance under high I/O workloads.

---

## 2. **What are the key differences in the database URL connection string when migrating from sync SQLAlchemy to async SQLAlchemy, and what driver changes are required for PostgreSQL?**
   For async SQLAlchemy with PostgreSQL, you need an async driver like `asyncpg`. The URL changes from:

```
postgresql://user:pass@host:port/dbname
```

to

```
postgresql+asyncpg://user:pass@host:port/dbname
```

The `+asyncpg` tells SQLAlchemy to use the async driver, enabling `async` sessions with `AsyncSession`.

---

## 3. **What is ASGI (Asynchronous Server Gateway Interface) and how does it differ from WSGI?**
   WSGI is a synchronous interface for Python web servers and apps, suitable for blocking code. ASGI is asynchronous, supporting `async`/`await`, WebSockets, and long-lived connections. ASGI allows concurrent handling of multiple requests without blocking threads, which is essential for modern async frameworks like FastAPI.

---

## 4. **How is FastAPI different from Flask?**

* FastAPI is **async-first** and uses Python type hints for automatic request validation, serialization, and OpenAPI generation.
* Flask is synchronous, minimalistic, and requires manual validation and documentation.
* FastAPI has built-in support for modern features like async routes, dependency injection, and automatic docs via Swagger/OpenAPI, making it better suited for high-performance APIs.

---

## 5. **Explain the difference between `response_model`, `response_model_exclude`, and `response_model_include` in FastAPI route decorators. When would you use each?**

* `response_model`: defines the Pydantic model used to serialize the response; validates and formats outgoing data.
* `response_model_include`: specifies which fields to include from the response model; useful for partial responses.
* `response_model_exclude`: specifies fields to exclude from the response; useful for hiding sensitive data like passwords.

Example use:

* `response_model` â†’ standard API output.
* `response_model_include` â†’ only return `id` and `name`.
* `response_model_exclude` â†’ exclude `password` or `secret_token`.

---

## 6. **How do you implement JWT authentication in FastAPI**

* Use `JWT`  to encode/dec ode tokens.
* Create a login route that generates a JWT with user info and expiry.
* Define a dependency that extracts and verifies the token from `Authorization` header.
* Use the dependency in protected routes to access user information.
* Example:

```python
from fastapi import Depends, HTTPException
from fastapi.security import OAuth2PasswordBearer
from jose import jwt, JWTError

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        user_id = payload.get("sub")
        if user_id is None:
            raise credentials_exception
        return {"id": user_id}
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
```
**General Procedure to Implement JWT Authentication in FastAPI:**

1. **Create a login endpoint** that accepts user credentials, like password or username.
2. **Verify credentials** against your database or user store, check the user exists in db or not, compare hash password.
3. **Generate a JWT token** with user info and expiration using a secret key and algorithm hs256.
4. **Return the token** to the client.
5. **Create a dependency** (e.g., `get_current_user`) that reads and validates the token from the `Authorization` header.
6. **Apply the dependency** to protected routes to enforce authentication.
7. **Optionally handle token expiration and refresh logic**.


---

## 7. **How does FastAPI handle synchronous functions differently?**

* FastAPI can run synchronous functions in a **thread pool** using `starlette.concurrency.run_in_threadpool`.
* Sync functions **block the thread** they're running in, but the event loop continues handling other async tasks in other threads.
* Async functions are preferred for I/O-bound operations, but sync functions still work for CPU-bound tasks or libraries without async support.

---
# session-12-django

## **What is Django's MTV pattern?**

Django follows the MTV (Model-Template-View) architecture:

* **Model**: Defines the data structure and database schema.
* **Template**: Handles presentation logic, rendering HTML to the user.
* **View**: Contains the business logic, handles requests, fetches data from models, and renders templates.

---

**What's the difference between blank=True and null=True?**

* **null=True**: Database-related; allows storing `NULL` in the database.
* **blank=True**: Form/validation-related; allows form fields to be left empty.
  Typically, for string-based fields, use `blank=True, null=False`.

---

## **What's the difference between auto_now and auto_now_add?**

* **auto_now_add=True**: Sets the field to the current timestamp when the object is first created.
* **auto_now=True**: Updates the field to the current timestamp every time the object is saved.

---

## **What are Django migrations and why are they important?**

* Migrations are Djangoâ€™s way of propagating model changes to the database schema.
* They allow version control of the database structure, easy schema updates, and consistency across environments.

---

## **What is the N+1 query problem? How to avoid it in Django?**

* **N+1 problem**: Querying a list of objects (N) and then accessing a related field causes an additional query per object, leading to N+1 queries.
* **Solution in Django**: Use `select_related` for foreign key/one-to-one relations or `prefetch_related` for many-to-many/one-to-many relations to fetch related objects efficiently.

---

## **What is the Meta class in Django models?**

* `Meta` is an inner class in a Django model used to define model metadata, such as:

    * Database table name (`db_table`)
    * Ordering of results (`ordering`)
    * Verbose name (`verbose_name`)
    * Unique constraints (`unique_together`)

---

## **What's the purpose of str() method in models?**

* The `__str__` method defines the human-readable string representation of a model instance, used in admin, shell, and templates for easier identification.


---

# Session14 - advanced web development 


## 1. Explain the WebSocket protocol and how it differs from HTTP polling and long polling

**Answer:**

1. **WebSocket åè®®**

    * å»ºç«‹ä¸€æ¬¡ **æŒä¹…åŒ–çš„å…¨åŒå·¥è¿æ¥**
    * å®¢æˆ·ç«¯ä¸æœåŠ¡å™¨éƒ½å¯ä»¥ä¸»åŠ¨å‘é€æ¶ˆæ¯
    * ä½¿ç”¨ HTTP æ¡æ‰‹å‡çº§åè®® â†’ ä¹‹åä¸å†éœ€è¦é‡å¤å»ºç«‹è¿æ¥
    * ä½å»¶è¿Ÿã€é«˜å®æ—¶æ€§ï¼ˆå¦‚èŠå¤©ã€åä½œç¼–è¾‘ã€è¡Œæƒ…æ¨é€ï¼‰

2. **ä¸ Pollingã€Long Polling çš„åŒºåˆ«**

    * **HTTP Polling**

        * å®¢æˆ·ç«¯å®šæœŸå‘è¯·æ±‚(å¦‚æ¯ 1s)
        * éå®æ—¶ï¼Œé«˜ç½‘ç»œå¼€é”€
    * **HTTP Long Polling**

        * å®¢æˆ·ç«¯å‘è¯·æ±‚ â†’ æœåŠ¡ç«¯ä¿æŒè¿æ¥ç›´åˆ°æœ‰æ•°æ®
        * åŠå®æ—¶ï¼Œä½†ä»æ˜¯ *request-response*
    * **WebSocket**

        * ä¸€æ¬¡è¿æ¥ï¼Œå¤šæ¬¡é€šä¿¡
        * çœŸæ­£çš„åŒå‘å®æ—¶æ¨é€
        * ç½‘ç»œå¼€é”€æœ€å°ã€å»¶è¿Ÿæœ€ä½

---

## 2. What caching strategy would you use for frequently accessed but slowly changing data

**Answer:**

1. **Cache-aside (Lazy Loading)** æœ€å¸¸ç”¨

    * è¯»å–ï¼šå…ˆæŸ¥ç¼“å­˜ï¼Œä¸åœ¨å†æŸ¥æ•°æ®åº“ â†’ æ”¾å…¥ç¼“å­˜
    * æ›´æ–°ï¼šå…ˆæ›´æ–°æ•°æ®åº“ï¼Œå†åˆ é™¤ç¼“å­˜
    * é€‚ç”¨äºï¼š**è¯»å¤šã€å†™å°‘ã€æ•°æ®å˜åŒ–æ…¢**
    * ç¤ºä¾‹ï¼šå•†å“è¯¦æƒ…é¡µã€é…ç½®ã€æ’è¡Œæ¦œ

2. å¯æ­é…ï¼š

    * **TTLï¼ˆè¿‡æœŸæ—¶é—´ï¼‰**
    * **å®šæœŸ refresh é¢„çƒ­**
    * **Redis + JSON serialization**

---

## 3. Explain the difference between cache-aside, write-through, and write-behind caching patterns

**Answer:**

1. **Cache-asideï¼ˆæ—è·¯ç¼“å­˜ï¼‰**

    * è¯»æ—¶åŠ è½½ï¼Œå†™æ—¶æ›´æ–° DB + åˆ é™¤ç¼“å­˜
    * ä¼˜ç‚¹ï¼šæœ€çµæ´»
    * ç¼ºç‚¹ï¼šå†™ä¸ä¸€è‡´é£é™©ï¼ˆå·²è¢«ä¸šç•Œæˆç†Ÿæ–¹æ¡ˆé¿å…ï¼‰

2. **Write-throughï¼ˆå†™ç©¿ï¼‰**

    * å†™ â†’ åŒæ—¶å†™ç¼“å­˜ & DB
    * æ•°æ®ç»å¯¹ä¸€è‡´
    * ç¼ºç‚¹ï¼šå†™æ€§èƒ½æ…¢

3. **Write-behindï¼ˆå†™å›ï¼‰**

    * å†™ â†’ å†™ç¼“å­˜ï¼›DB åœ¨åå°å¼‚æ­¥åˆ·æ–°
    * å†™æ€§èƒ½æœ€å¿«
    * ç¼ºç‚¹ï¼šDB ä¸ç¼“å­˜**å¼ºä¸€è‡´æ€§æ— æ³•ä¿è¯**

---

## 4. Describe the differences between RabbitMQ, Kafka, and SQS

**Answer:**

1. **RabbitMQ**

    * æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆMQï¼‰
    * å¼ºè°ƒ**å¯é æŠ•é€’ã€å¤æ‚è·¯ç”±ï¼ˆexchangeï¼‰ã€ç¡®è®¤æœºåˆ¶**
    * é€‚ç”¨äºï¼šçŸ­æ¶ˆæ¯ã€ä»»åŠ¡é˜Ÿåˆ—ã€åŒæ­¥è§£è€¦

2. **Kafka**

    * åˆ†å¸ƒå¼æµå¤„ç†å¹³å°
    * å¼ºè°ƒ**é«˜ååã€å¯æŒä¹…åŒ–æ—¥å¿—å­˜å‚¨ã€åˆ†åŒº**
    * é€‚ç”¨äºï¼šæ—¥å¿—ã€äº‹ä»¶æµã€çœŸå®æ—¶é—´å¤§æ•°æ®å¤„ç†

3. **AWS SQS**

    * æ‰˜ç®¡é˜Ÿåˆ—æœåŠ¡
    * ç®€å•ã€æ— éœ€ç®¡ç† Broker
    * é€‚ç”¨äºï¼šäº‘åŸç”Ÿå¾®æœåŠ¡è§£è€¦ï¼Œä½ç»´æŠ¤æˆæœ¬

---

## 5. What are message queues and why are they important in distributed systems?

**Answer:**

1. **å®šä¹‰**

    * å¼‚æ­¥é€šä¿¡æœºåˆ¶ï¼Œé€šè¿‡é˜Ÿåˆ—ä¼ é€’æ¶ˆæ¯ï¼Œä¸è¦æ±‚åŒæ–¹åŒæ—¶åœ¨çº¿

2. **é‡è¦æ€§**

    * **è§£è€¦**ï¼šæœåŠ¡ç‹¬ç«‹
    * **å‰Šå³°å¡«è°·**ï¼šç¼“å†²æµé‡
    * **å¼‚æ­¥ä»»åŠ¡**ï¼šæé«˜å“åº”é€Ÿåº¦
    * **é‡è¯•æœºåˆ¶**ï¼šæé«˜å¯é æ€§
    * **å®¹é”™æ€§**ï¼šæœåŠ¡å®•æœºä¸ä¼šä¸¢æ¶ˆæ¯ï¼ˆè§† MQ å…·ä½“é…ç½®ï¼‰

---

## 6. How would you implement a retry mechanism with exponential backoff for failed message processing?

**Answer:**

1. **æ­¥éª¤**

    * æ•è·å¤„ç†å¤±è´¥
    * é‡è¯•æ¬¡æ•°é™åˆ¶ï¼ˆå¦‚ 3 æ¬¡ï¼‰
    * ä½¿ç”¨ **2^n Ã— baseDelay** è¿›è¡Œç­‰å¾…
    * é‡è¯•ä»å¤±è´¥ â†’ è¿›å…¥ DLQ

2. **ä¼ªä»£ç **

```python
delay = base
for attempt in range(max_retries):
    try:
        process(msg)
        break
    except:
        sleep(delay)
        delay *= 2
else:
    send_to_dlq(msg)
```

---

## 7. Explain the concept of dead letter queues and when you'd use them

**Answer:**

1. **å®šä¹‰**

    * Dead Letter Queue (DLQ) æ˜¯å­˜æ”¾ **å¤„ç†å¤±è´¥æ¶ˆæ¯** çš„ç‰¹æ®Šé˜Ÿåˆ—

2. **ç”¨é€”**

    * é‡è¯•è¶…è¿‡æ¬¡æ•°
    * æ¶ˆæ¯æ ¼å¼é”™è¯¯
    * æ— æ³•è¢«æ¶ˆè´¹è€…å¤„ç†
    * éœ€è¦äººå·¥æ£€æŸ¥ä¸ä¿®å¤

3. **é‡è¦æ€§**

    * ç¡®ä¿ç³»ç»Ÿä¸å› ä¸ºåæ¶ˆæ¯è€Œå¡æ­»
    * æä¾›å¯å®¡è®¡æ€§
    * ç¡®ä¿ä¸»é˜Ÿåˆ—æ€§èƒ½ä¸å—æŸ

---

## 8. How does FastAPI handle synchronous functions differently?

**Answer:**

1. **Syncï¼ˆæ™®é€šå‡½æ•°ï¼‰**

    * FastAPI ä¼šåœ¨ **ThreadPoolExecutor** ä¸­è¿è¡Œ
    * é€‚ç”¨äºï¼šCPU-boundã€é˜»å¡ IO çš„ä»£ç 
    * æ¯ä¸ªè¯·æ±‚å ç”¨ä¸€ä¸ªçº¿ç¨‹

2. **Asyncï¼ˆasync defï¼‰**

    * è¿è¡Œåœ¨ event loop
    * é€‚ç”¨äºï¼šéé˜»å¡ IOã€HTTP è°ƒç”¨
    * å¹¶å‘æ›´é«˜ï¼Œèµ„æºæ¶ˆè€—æ›´ä½

3. **FastAPI åŒºåˆ«å¤„ç†**

    * æ ¹æ®å‡½æ•°ç±»å‹è‡ªåŠ¨é€‰æ‹©æ‰§è¡Œç­–ç•¥
    * é¿å…é˜»å¡ event loop

---

## English Version


1. **Explain the WebSocket protocol and how it differs from HTTP polling and long polling**

**Answer:**
WebSocket is a full-duplex communication protocol built on top of TCP. After an initial HTTP handshake, the connection upgrades to WebSocket and stays open, allowing both client and server to push messages at any time.

**Key differences:**

1. **HTTP Polling**
    * Client ask server repeatedly at fixed intervals
    * Workflow
      * client send request for concrete endpoints
      * server instantly receive and responds even if no data udpated 
      * client waits for seconds then repeats
    * pros: easy to implement and works scalable 
    * cons: many useless requests to fetch old data, high latency, waste bandwidth and server resources 
2. **Long Polling**
    * Client sends a request and the server holds it until new data is available or timeout 
    * Workflow: 
      * client send request 
      * server waits until resource ready then repsonds
      * client send new request again 
    * pros
      * near real-time updates 
      * far fewer empty responses than short polling 
    * cons
      * server keeps connection open 
      * harder to implement 
      * still no true push -> client must be re-request
    * Used when needs real-time ish updates or no websockets 
3. **WebSocket**
    * One persistent connection.
    * No repeated HTTP overhead.
    * Real-time, bidirectional updates.
4. Modern Push
   1. WebSocket: full-duplex persistent connection
   2. SSE(Server-Sent Events): server push over HTTP 

> The holding open period does not inherently add latency, if a message arrives while request is pending

---

1. **What caching strategy would you use for frequently accessed but slowly changing data?**

**Answer:**
Use a **cache-aside strategy with TTL (time-to-live)**.

**Why this fits:**

* The data is read often â†’ caching avoids unnecessary database hits.
* The data changes infrequently â†’ TTL ensures cache eventually refreshes.
* Cache-aside gives full control over when to reload.

Typical TTL: **5â€“30 minutes**, depending on consistency requirements.

**Caching Strategy**

Caching is a storing frequently accessed data in fast-access layer( memory, local storage, cdn) so that future requests doesn't hit slower backend (disk, db, api )

| Name                          | Workflow                                                                      | Pros                    | Cons                                 | Usage Condition (UC)                                                         |
| ----------------------------- | ----------------------------------------------------------------------------- | ----------------------- | ------------------------------------ | ---------------------------------------------------------------------------- |
| Write-through                 | App writes â†’ Cache stores â†’ DB writes                                         | Cache always up-to-date | Slower writes                        | Updating a user profile: write to DB and Redis simultaneously                |
| Write-back (Write-behind)     | App writes â†’ Cache stores â†’ DB deferred                                       | Faster writes           | Risk of losing data if cache crashes | Session updates in a gaming server                                           |
| Read-through                  | App reads â†’ Cache hit? â†’ Miss â†’ Cache loads from DB â†’ Return data             | Simplifies code         | Slightly slower first access         | Product info lookup in e-commerce                                            |
| Cache-aside (Lazy loading)    | App reads â†’ Cache hit? â†’ Miss â†’ App queries DB â†’ App writes to cache â†’ Return | Flexible                | Code must handle caching             | Fetch user profile: check Redis â†’ if miss, query PostgreSQL â†’ populate Redis |
| Time-to-Live (TTL)/Expiration | Cache stores â†’ Data expires automatically after TTL â†’ Remove                  | Prevents stale data     | Must tune TTL                        | News feed caching: expire every 5 minutes                                    |
| Least Recently Used (LRU)     | Cache stores â†’ Evict least recently used items when full                      | Efficient memory usage  | Can evict useful data                | Redis LRU eviction policy                                                    |


**Read-Through VS Caching-Aside**
Both two has similar workflow:
App checks cache â†’ miss â†’ fetch from DB â†’ populate cache â†’ return

Core differenceï¼š
- Cache Aside: Application/Developer needs to manage caching reload and fetch using mannual query, flexible to add business logic like logging, authentication, authorization, data encapsulation 
- Read-Through: `cache.get_or_load(user_id)` to auto-search and load db data, encapsulated well


---

1. **Explain the difference between cache-aside, write-through, and write-behind patterns**

**Answer:**

1. **Cache-Aside (Lazy Loading)**

    * Application reads from cache first.
    * If missing â†’ fetch from DB â†’ store in cache.
    * Writes go to DB first, cache is updated or invalidated.
    * Pros: simple, flexible.
    * Cons: first read is slow (â€œcache missâ€).

2. **Write-Through**

    * All writes go to cache first; cache immediately writes to DB.
    * Read always comes from cache.
    * Pros: data in cache is always consistent.
    * Cons: writes become slower.

3. **Write-Behind (Write-Back)**

    * Application writes to cache only; cache asynchronously flushes changes to DB.
    * Pros: very fast writes.
    * Cons: risk of data loss if cache dies before flushing.

---

4. **Describe RabbitMQ, Kafka, and SQS in terms of use cases and guarantees**

**Answer:**
**RabbitMQ**

* **Type:** Message broker, queue-based
* **Use cases:** Task queues, request/response, events requiring ordering
* **Guarantee:** At-least-once delivery, routing flexibility

**Kafka**

* **Type:** Distributed log streaming platform
* **Use cases:** High-throughput event streaming, logs, analytics pipelines
* **Guarantee:** Durable log with partition ordering; consumer groups

**SQS (AWS)**

* **Type:** Fully managed queue service
* **Use cases:** Simple decoupling between microservices
* **Guarantee:** At-least-once; FIFO option available

---

5. **What are message queues and why are they important in distributed systems?**

**Answer:**
Message queues decouple producers and consumers by allowing messages to be stored temporarily until processed.

**Importance:**

* Smooth out traffic spikes
* Prevent service overload
* Improve fault tolerance
* Enable asynchronous workflows
* Increase scalability and resilience

Examples: task processing, event notifications, job scheduling.

---

6. **How would you implement a retry mechanism with exponential backoff for failed message processing?**

**Answer:**
Use a retry loop with a delay that grows exponentially after each failure.

**Example logic:**

1. Try to process the message.
2. On failure â†’ wait `base_delay * (2^attempt)` seconds.
3. Retry up to a maximum attempt count.
4. If still failing â†’ send to a Dead Letter Queue.

**Example pattern:**

* 1s â†’ 2s â†’ 4s â†’ 8s â†’ â€¦
* With jitter to avoid retry storms.

---

7. **Explain dead letter queues and when you'd use them**

**Answer:**
A Dead Letter Queue (DLQ) holds messages that could not be processed successfully after multiple retries.

**Use cases:**

* Messages with malformed data
* Infinite retry prevention
* Operational debugging
* Monitoring abnormal behavior
* Ensuring the main queue stays clean

DLQs improve reliability and prevent system clogging.

---

8. **How does FastAPI handle synchronous functions differently?**

**Answer:**
FastAPI automatically executes synchronous functions (`def`) in a threadpool managed by Starlette.

**Details:**

* Synchronous functions run in a worker thread â†’ do not block the event loop
* Asynchronous functions (`async def`) run directly on the event loop
* FastAPI chooses the correct execution model based on the function signature

This enables CPU-bound or blocking operations to run without affecting overall concurrency.

---

# Session-15-graphQL

## **What is GraphQL and how does it differ from REST? What problems does it solve?**

**English:**

1. GraphQL is a query language and runtime that lets clients request exactly the data they need.
2. Unlike RESTâ€”which exposes multiple endpointsâ€”GraphQL exposes a single endpoint where clients specify the shape of the response.
3. It solves problems such as over-fetching (receiving more data than needed) and under-fetching (requiring multiple requests to gather data).
4. It is especially useful for complex frontends and mobile apps needing efficient data loading.

**Chinese:**

1. GraphQL æ˜¯ä¸€ç§æŸ¥è¯¢è¯­è¨€ä¸è¿è¡Œæ—¶ï¼Œå…è®¸å®¢æˆ·ç«¯â€œç²¾ç¡®åœ°â€è¯·æ±‚æ‰€éœ€è¦çš„æ•°æ®ç»“æ„ã€‚
2. ä¸éœ€è¦å¤šä¸ª endpoint çš„ REST ä¸åŒï¼ŒGraphQL åªæä¾›ä¸€ä¸ª endpointï¼Œå®¢æˆ·ç«¯è‡ªå·±å®šä¹‰è¿”å›æ•°æ®çš„ç»“æ„ã€‚
3. å®ƒè§£å†³äº† REST å¸¸è§çš„ **æ•°æ®è¿‡åº¦è·å–**ï¼ˆæ‹¿äº†å¤ªå¤šï¼‰å’Œ **æ•°æ®è·å–ä¸è¶³**ï¼ˆéœ€è¦å¤šæ¬¡è¯·æ±‚ï¼‰çš„é—®é¢˜ã€‚
4. åœ¨å‰ç«¯å¤æ‚æˆ–ç§»åŠ¨ç«¯éœ€è¦é«˜æ•ˆèŠ‚æµæ•°æ®é‡çš„åœºæ™¯ä¸‹ç‰¹åˆ«æœ‰ä»·å€¼ã€‚

---

## **What is the N+1 query problem in GraphQL and how can it be resolved?**

**English:**

1. N+1 happens when a resolver fetches data for each item individually, causing one query for the list + N queries for each item.
2. Example: fetching a list of posts, then fetching the author for each post.
3. It is resolved using batching and caching techniques, most commonly via **DataLoader**, which groups many small queries into one.

**Chineseï¼š**

1. N+1 é—®é¢˜æŒ‡ï¼šGraphQL resolver å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ åˆ†åˆ«æŸ¥è¯¢ï¼Œä¾‹å¦‚å…ˆæŸ¥ä¸€æ¬¡ posts åˆ—è¡¨ï¼Œå†å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ª post æŸ¥ä¸€æ¬¡ authorã€‚
2. å¯¼è‡´æ€»å…± 1 + N æ¬¡æŸ¥è¯¢ï¼Œæ€§èƒ½éå¸¸å·®ã€‚
3. è§£å†³æ–¹å¼æ˜¯ **æ‰¹å¤„ç†ï¼ˆbatchingï¼‰ä¸ç¼“å­˜ï¼ˆcachingï¼‰**ï¼Œæœ€å¸¸ç”¨å·¥å…·æ˜¯ **DataLoader**ï¼Œå®ƒä¼šæŠŠå¤šä¸ªç‹¬ç«‹æŸ¥è¯¢åˆå¹¶æˆâ€œä¸€æ¬¡æŸ¥è¯¢â€ã€‚

---

## **Describe the difference between nullable and non-nullable fields in GraphQL. How do you denote them in the schema?**

**English:**

1. Nullable fields can return null; non-nullable fields must always return a value.
2. Non-nullable types are marked with an exclamation mark `!`.
3. Example:

    * `name: String` â†’ may return null
    * `name: String!` â†’ guaranteed non-null

**Chineseï¼š**

1. å¯ç©ºå­—æ®µå¯ä»¥è¿”å› nullï¼›ä¸å¯ç©ºå­—æ®µå¿…é¡»è¿”å›å®é™…å€¼ã€‚
2. ä¸å¯ç©ºå­—æ®µåœ¨ schema ä¸­ç”¨ `!` æ ‡è®°ã€‚
3. ç¤ºä¾‹ï¼š

    * `name: String` â†’ å¯ä»¥è¿”å› null
    * `name: String!` â†’ ä¸€å®šä¸èƒ½æ˜¯ null

---

## **What is the DataLoader pattern and why is it important for GraphQL performance?**

**English:**

1. DataLoader batches multiple similar requests into a single query and caches results during a request cycle.
2. It prevents N+1 queries by grouping multiple loads for the same resource.
3. It improves both DB efficiency and backend throughput.

**Chineseï¼š**

1. DataLoader ä¼šå°†å¤šä¸ªç›¸ä¼¼çš„æ•°æ®è¯·æ±‚ï¼ˆä¾‹å¦‚æ ¹æ®å¤šä¸ª id æŸ¥å¤šä¸ªç”¨æˆ·ï¼‰åˆå¹¶æˆâ€œä¸€æ¬¡æ‰¹é‡æŸ¥è¯¢â€ï¼Œå¹¶åœ¨ä¸€æ¬¡è¯·æ±‚ç”Ÿå‘½å‘¨æœŸä¸­åšç¼“å­˜ã€‚
2. å®ƒèƒ½é¿å… N+1 æŸ¥è¯¢ï¼ŒæŠŠå¤§é‡å°æŸ¥è¯¢å˜æˆå°‘é‡æ‰¹é‡æŸ¥è¯¢ã€‚
3. ä½œç”¨æ˜¯æ˜¾è‘—æå‡æ•°æ®åº“æ•ˆç‡å’Œåç«¯ååé‡ã€‚

---

## **What are GraphQL fragments and when would you use them?**

**English:**

1. Fragments allow reusing shared field selections across multiple queries.
2. They avoid repeated boilerplate and ensure consistency.
3. Example use case: multiple components needing the same user fields.

**Chineseï¼š**

1. Fragment ç”¨äºå¤ç”¨é‡å¤çš„å­—æ®µé€‰æ‹©ï¼Œè®©å¤šä¸ª query èƒ½å…±äº«åŒä¸€æ®µå­—æ®µå®šä¹‰ã€‚
2. å®ƒå‡å°‘é‡å¤ä»£ç å¹¶ä¿æŒå­—æ®µä¸€è‡´æ€§ã€‚
3. å…¸å‹åœºæ™¯ï¼šå¤šä¸ªå‰ç«¯ç»„ä»¶éƒ½éœ€è¦ User çš„éƒ¨åˆ†å…±åŒå­—æ®µã€‚

---

## **How would you implement pagination in a Python GraphQL API?**

**English:**

1. Two common patterns: **offset-based** and **cursor-based** pagination.
2. Offset: pass offset/limit to the DBï¼Œ similar with SQL syntax (simple but less scalable).
3. Cursor: use stable encoded cursors, usually encoded id or timestamp to mark position (better for large or frequently updated data).
4. workflow
Client sends pagination arguments
â†’ GraphQL resolver receives args
â†’ Resolver queries DB with pagination logic
â†’ Resolver formats response with page info
â†’ Client renders page and requests next page

offset:
```graphql
type User {
  id: ID!
  name: String!
}

type Query {
  users(limit: Int!, offset: Int!): [User!]!
}
```


**Chineseï¼š**

1. åˆ†é¡µæœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š**offset åˆ†é¡µ**ä¸ **cursor åˆ†é¡µ**ã€‚
2. Offsetï¼šé€šè¿‡ offset/limit æ§åˆ¶ï¼Œç®€å•ä½†å¯¹å¤§æ•°æ®æˆ–é¢‘ç¹æ›´æ–°çš„è¡¨ä¸ç¨³å®šã€‚
3. Cursorï¼šåŸºäºæ¸¸æ ‡ï¼ˆç¨³å®šå­—æ®µï¼Œä¾‹å¦‚ IDï¼‰ï¼Œæ›´é€‚åˆå¤§è§„æ¨¡æ•°æ®åˆ†é¡µã€‚
4. åœ¨ Python GraphQLï¼ˆGraphene / Strawberryï¼‰ä¸­ï¼š

    * å®šä¹‰åˆ†é¡µå‚æ•°ï¼ˆå¦‚ `first`, `after`ï¼‰
    * æ ¹æ® cursor æŸ¥è¯¢
    * è¿”å› edges + pageInfoï¼Œç¬¦åˆ Relay åˆ†é¡µè§„èŒƒã€‚

---

## **How do you handle errors and exceptions in GraphQL resolvers?**

**English:**

1. GraphQL never throws raw server errors to the client; instead, it returns partial data + an `errors` array.
2. In Python, wrap resolver logic in try/except and raise GraphQLError for user-friendly errors.
3. This keeps the response structure predictable and prevents breaking the entire query when only one field fails.

**Chineseï¼š**

1. GraphQL çš„é”™è¯¯ä¸ä¼šå¯¼è‡´æ•´ä¸ªè¯·æ±‚ç›´æ¥å¤±è´¥ï¼Œè€Œæ˜¯è¿”å›éƒ¨åˆ†æ•°æ®å¹¶å¸¦ä¸Š `errors` æ•°ç»„ã€‚
2. åœ¨ Python å®ç°ä¸­ï¼Œå¯åœ¨ resolver ä¸­ä½¿ç”¨ try/exceptï¼Œå¹¶æŠ›å‡º GraphQLError æ¥è¿”å›å¯è¯»é”™è¯¯ã€‚
3. å¥½å¤„æ˜¯ï¼šä¸€ä¸ªå­—æ®µå¤±è´¥ä¸ä¼šå¯¼è‡´æ•´ä¸ª query å´©æºƒï¼Œå¢å¼º API é²æ£’æ€§ã€‚


---

# Session-16-ci-cd-unit-test

## 1. Whatâ€™s the difference between unit tests, integration tests, and end-to-end tests? When would you use each?

### **English**

**Unit tests**

* Test a single function/class in isolation.
* Fast, small scope, no external systems.
  **Use when:** verifying core logic correctness.

**Integration tests**

* Test how multiple components work together (e.g., service + DB layer).
* Medium speed.
  **Use when:** validating interfaces bw components.

**End-to-end (E2E) tests**

* Test the full system from user perspective, including network, DB, auth.
* Slowest but highest confidence.
  **Use when:** validating full real-world scenarios.

### **ä¸­æ–‡**

**å•å…ƒæµ‹è¯•**

* æµ‹è¯•æœ€å°é€»è¾‘å•å…ƒï¼ˆå‡½æ•°/ç±»ï¼‰ï¼Œå®Œå…¨éš”ç¦»å¤–éƒ¨ä¾èµ–ã€‚
* å¿«ã€è¦†ç›–é€»è¾‘ç»†èŠ‚ã€‚
  **ä½¿ç”¨åœºæ™¯ï¼š** æ ¸å¿ƒç®—æ³•ã€ä¸šåŠ¡é€»è¾‘æ­£ç¡®æ€§ã€‚

**é›†æˆæµ‹è¯•**

* æµ‹è¯•å¤šä¸ªæ¨¡å—å¦‚ä½•ååŒå·¥ä½œï¼ˆä¾‹å¦‚ï¼šService + Repositoryï¼‰ã€‚
* é€Ÿåº¦ä¸­ç­‰ã€‚
  **ä½¿ç”¨åœºæ™¯ï¼š** éªŒè¯æ¨¡å—æ¥å£å’Œæ•°æ®æµã€‚

**ç«¯åˆ°ç«¯æµ‹è¯•**

* æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºï¼Œæµ‹è¯•æ•´ä¸ªç³»ç»Ÿé“¾è·¯ï¼ˆç½‘ç»œã€æ•°æ®åº“ã€æƒé™ï¼‰ã€‚
* æœ€æ…¢ä½†æœ€çœŸå®ã€‚
  **ä½¿ç”¨åœºæ™¯ï¼š** å›å½’æµ‹è¯•ã€ä¸Šçº¿å‰éªŒæ”¶ã€‚

---

## ğŸ”´ 2. Explain the purpose of mocking in unit tests. Difference between Mock, MagicMock, and patch.

### **English**

**Purpose of mocking**

* Replace external dependencies (DB, API, file I/O) so unit tests run fast and deterministically.

**Mock**

* Basic mock object that tracks calls, arguments, return values.

**MagicMock**

* Extends Mock and adds â€œmagic methodsâ€ (e.g., `__len__`, `__iter__`, `__getitem__`), useful for mocking objects that behave like collections.
* Fully manage fake object's returned value, implementation when system support dependency injection

**patch**

* Temporarily replaces an object in module (`function`, `class`, `variable`) during test, injects `MagicMock` by default, restored after test 
* Used when you want to mock object methods at the correct import path of other modules.
* Supports auto-restore, auto-replaced, clean scope than `MagicMock`  

### **ä¸­æ–‡**

**Mock çš„ç›®çš„**

* å±è”½å¤–éƒ¨ä¾èµ–ï¼ˆæ•°æ®åº“ã€HTTPã€æ–‡ä»¶ç³»ç»Ÿï¼‰ï¼Œè®©å•æµ‹åªå…³æ³¨é€»è¾‘æœ¬èº«å¹¶ä¸”æ‰§è¡Œæ›´å¿«ã€æ›´ç¨³å®šã€‚

**Mock**

* åŸºç¡€ mock å¯¹è±¡ï¼Œå¯è®°å½•è°ƒç”¨æ¬¡æ•°ã€å‚æ•°ã€è¿”å›å€¼ã€‚

**MagicMock**

* Mock çš„å¢å¼ºç‰ˆï¼Œæ”¯æŒå„ç§â€œé­”æœ¯æ–¹æ³•â€ï¼Œé€‚åˆ mock åˆ—è¡¨/å­—å…¸/è¿­ä»£å™¨ç­‰å¯¹è±¡ã€‚

**patch**

* åŠ¨æ€æ›¿æ¢æ¨¡å—ä¸­çš„å¯¹è±¡ï¼Œç”¨æ¥ mock å‡½æ•°ã€ç±»ã€å˜é‡ã€‚
* é‡ç‚¹ï¼šå¿…é¡» patch **ä»£ç å®é™… import çš„è·¯å¾„**ã€‚

---

## 3. Explain test coverage. What's a good percentage?

### **English**

**Test coverage**

* A metric showing the percentage of executed code during tests (lines, branches, functions).

**Good coverage target:**

* 80% is common industry baseline.
* 100% is unnecessaryâ€”high coverage doesnâ€™t guarantee quality.

### **ä¸­æ–‡**

**æµ‹è¯•è¦†ç›–ç‡**

* è¡¨ç¤ºæµ‹è¯•è¿‡ç¨‹ä¸­è¢«æ‰§è¡Œçš„ä»£ç å æ€»ä»£ç çš„æ¯”ä¾‹ï¼ˆè¡Œè¦†ç›–ã€åˆ†æ”¯è¦†ç›–ã€å‡½æ•°è¦†ç›–ï¼‰ã€‚

**æ¯”è¾ƒåˆç†çš„ç›®æ ‡ï¼š**

* è¡Œä¸šæ™®éæ ‡å‡†ï¼š80%ã€‚
* ä¸éœ€è¦è¿½æ±‚ 100%ï¼Œå…³é”®æ˜¯æµ‹è¯•è´¨é‡è€Œä¸æ˜¯æ•°å­—ã€‚

---

## 4. How do you test code involving databases without hitting real DBs?

### **English**

Strategies:

1. **Mock the database layer** (replace queries with fakes).
2. **Use an in-memory DB** (SQLite for lightweight tests).
3. **Use test containers** (Dockerized Postgres/MySQL specifically for integration tests).
4. **Repository pattern** to isolate DB logic for easier mocking.

### **ä¸­æ–‡**

æµ‹è¯•æ•°æ®åº“ç›¸å…³ä»£ç çš„ç­–ç•¥ï¼š

1. **Mock æ•°æ®è®¿é—®å±‚**ï¼ˆæŠŠæ•°æ®åº“è°ƒç”¨æ›¿æ¢æˆå‡å¯¹è±¡ï¼‰ã€‚
2. **ä½¿ç”¨å†…å­˜æ•°æ®åº“**ï¼ˆå¦‚ SQLiteï¼‰åŠ å¿«æµ‹è¯•ã€‚
3. **ä½¿ç”¨ Testcontainers**ï¼ˆç”¨ Docker å¯åŠ¨çœŸå®ä½†ç‹¬ç«‹çš„æ•°æ®åº“è¿›è¡Œé›†æˆæµ‹è¯•ï¼‰ã€‚
4. **ä½¿ç”¨ Repository æŠ½è±¡å±‚**ï¼Œä¾¿äºéš”ç¦»å’Œ mockã€‚

---

## 5. Whatâ€™s test-driven development (TDD)?

### **English**

TDD cycle:

1. **Write a failing test** (Red)
2. **Write minimal code to make it pass** (Green)
3. **Refactor** while keeping tests green

Goal: clean design, high test coverage, clear requirements.

### **ä¸­æ–‡**

TDD å·¥ä½œæµç¨‹ï¼š

1. **å…ˆå†™å¤±è´¥çš„æµ‹è¯•**ï¼ˆçº¢ï¼‰
2. **å†™æœ€å°‘ä»£ç è®©æµ‹è¯•é€šè¿‡**ï¼ˆç»¿ï¼‰
3. **é‡æ„ä»£ç **ï¼Œæ‰€æœ‰æµ‹è¯•ä¿æŒé€šè¿‡

ç›®æ ‡ï¼šæ›´å¥½çš„è®¾è®¡ã€å¤©ç„¶é«˜è¦†ç›–ç‡ã€æ›´æ¸…æ™°çš„éœ€æ±‚ã€‚

---

## 6. Explain the typical stages in a CI/CD pipeline.

### **English**

Stages:

1. **Source / Checkout** â€“ pull code from repo
2. **Build** â€“ compile, package, install dependencies
3. **Unit Tests** â€“ fast logic validation
4. **Integration Tests** â€“ service + DB tests
5. **Static analysis** â€“ lint, type check, security scan
6. **Artifact packaging** â€“ build Docker image / wheel
7. **Deploy** â€“ to staging â†’ production
8. **Post-deploy tests** â€“ smoke tests/health checks

### **ä¸­æ–‡**

CI/CD æµæ°´çº¿å…¸å‹é˜¶æ®µï¼š

1. **æºç æ‹‰å–**
2. **æ„å»ºä¾èµ–/ç¼–è¯‘**
3. **å•å…ƒæµ‹è¯•**
4. **é›†æˆæµ‹è¯•**
5. **é™æ€åˆ†æï¼ˆlintã€ç±»å‹ã€å®‰å…¨æ‰«æï¼‰**
6. **åˆ¶å“æ‰“åŒ…ï¼ˆDocker é•œåƒã€wheel åŒ…ï¼‰**
7. **éƒ¨ç½²ï¼ˆæµ‹è¯•ç¯å¢ƒ â†’ ç”Ÿäº§ç¯å¢ƒï¼‰**
8. **éƒ¨ç½²åæ ¡éªŒï¼ˆå¥åº·æ£€æŸ¥ã€å†’çƒŸæµ‹è¯•ï¼‰**

---

## 7. Whatâ€™s the purpose of environment variables & secrets management in CI/CD?

### **English**

Purpose:

* Keep configuration out of code
* Securely manage sensitive data (DB passwords, API keys)
* Support environment-specific configs (dev/staging/prod)

How to handle secrets:

* Secrets Manager (AWS/GCP/Azure)
* Vault
* CI/CD encrypted variables
* Kubernetes Secrets

### **ä¸­æ–‡**

ç›®çš„ï¼š

* é…ç½®ä¸ä»£ç åˆ†ç¦»
* å®‰å…¨ç®¡ç†æ•æ„Ÿä¿¡æ¯ï¼ˆæ•°æ®åº“å¯†ç ã€API Keyï¼‰
* åŒºåˆ†ä¸åŒç¯å¢ƒçš„é…ç½®ï¼ˆå¼€å‘/æµ‹è¯•/ç”Ÿäº§ï¼‰

ç®¡ç†æ–¹å¼ï¼š

* äº‘å‚å•† Secrets Manager
* Vault
* CI/CD çš„åŠ å¯†å˜é‡
* Kubernetes Secrets

---

## 8. Explain Scrum roles: Product Owner, Scrum Master, Development Team.

### **English**

**Product Owner**

* Owns the product vision
* Prioritizes backlog
* Defines requirements and acceptance criteria

**Scrum Master**

* Facilitates meetings
* Removes blockers
* Ensures Scrum practices

**Development Team**

* Implements features
* Self-organizing
* Responsible for delivering increments

### **ä¸­æ–‡**

**äº§å“è´Ÿè´£äººï¼ˆPOï¼‰**

* è´Ÿè´£äº§å“æ„¿æ™¯
* ç»´æŠ¤ backlog ä¼˜å…ˆçº§
* å®šä¹‰éœ€æ±‚å’ŒéªŒæ”¶æ ‡å‡†

**Scrum Master**

* ä¸»æŒä¼šè®®
* ç§»é™¤å›¢é˜Ÿé˜»ç¢
* ä¿éšœ Scrum æµç¨‹æ­£ç¡®æ‰§è¡Œ

**å¼€å‘å›¢é˜Ÿ**

* å®ç°åŠŸèƒ½
* è‡ªç»„ç»‡åä½œ
* è´Ÿè´£æ¯ä¸ªè¿­ä»£çš„å¯äº¤ä»˜æˆæœ

---
# GenAI Day2 

**Q1. What is a text embedding and why is it useful for RAG?**

* Converts text into numeric vectors that capture semantic meaning
* Enables semantic search rather than exact keyword matching
* Allows retrieval even when wording differs between query and documents
* Serves as the foundation for grounding LLM answers in retrieved context
* **Pros:** robust to paraphrasing, scalable similarity search
* **Cons / reminders:** embeddings are static snapshots of meaning and may miss very fine-grained intent without good chunking or reranking


**Q2. What does embedding dimension mean, and why does it matter?**

* Indicates how many numeric features represent each text chunk
* Higher dimensions capture richer semantics and nuance
* Lower dimensions reduce memory, latency, and cost
* **Pros:** higher dimensions usually improve recall on complex content
* **Cons / reminders:** diminishing returns beyond a point; larger vectors increase storage and comparison cost


**Q3. How do you choose an embedding model for a RAG system?**

* Based on document domain, query complexity, latency needs, and budget
* Stronger models improve recall on technical or ambiguous queries
* Lighter models work better for high-throughput, low-latency systems
* **Pros:** flexibility to optimize for accuracy or cost
* **Cons / reminders:** model choice should be empirically tested; theoretical strength doesnâ€™t always translate to better retrieval

**Q4. Why use a vector database instead of FAISS directly?**

* FAISS handles fast nearest-neighbor search only
* Vector databases add metadata filtering, auth, sharding, backups, APIs
* Provide operational tooling needed for production environments
* **Pros:** faster time-to-production, safer scaling
* **Cons / reminders:** added operational cost and potential vendor lock-in compared to raw FAISS


**Q5. What role does metadata play? How does it help with access control?**

* Adds contextual attributes like document source, user permissions, or tags
* Enables filtering so users retrieve only authorized content
* Improves precision by scoping retrieval to relevant subsets
* **Pros:** better security and higher-quality retrieval
* **Cons / reminders:** poorly designed metadata schemas can limit flexibility or hurt recall



**Q6. Why is naÃ¯ve PDF/HTML parsing not enough?**

* Flattens structure and mixes unrelated text (tables, headers, footers)
* Introduces noise that degrades embedding quality
* Leads to incorrect or hallucinated answers in RAG
* **Pros of advanced parsing:** cleaner chunks, better semantic alignment
* **Cons / reminders:** layout-aware parsing is more complex and slower


**Q7. How does your system represent documents internally before building the index?**

* Models documents as a hierarchy: pages â†’ blocks â†’ chunks
* Preserves semantic and structural boundaries
* Produces embeddings that represent coherent ideas
* **Pros:** improved retrieval accuracy and explainability
* **Cons / reminders:** more preprocessing complexity and pipeline maintenance


**Q8. How does your system handle tables and charts?**

* Detects and extracts structured representations instead of raw text
* Preserves relationships between rows, columns, and labels
* Keeps descriptive context for semantic queries
* **Pros:** accurate retrieval for numeric or structured questions
* **Cons / reminders:** chart extraction can be lossy if underlying data is unavailable


**Q9. How did you design your chunking strategy, and an example where bad chunking failed?**

* Ensures chunks are self-contained and semantically complete
* Avoids splitting concepts or merging unrelated sections
* Poor chunking once fragmented a policy section, harming retrieval
* **Pros:** directly improves recall and answer grounding
* **Cons / reminders:** chunk size is a trade-offâ€”too large hurts precision, too small loses context


**Q10. Walk me through your full indexing pipeline.**

* Parse raw documents with layout awareness
* Clean text and organize into structured blocks
* Chunk content and attach metadata
* Embed chunks and store them in a vector database
* **Pros:** traceable, scalable, and retrieval-optimized pipeline
* **Cons / reminders:** ingestion pipelines must handle re-indexing, versioning, and data drift over time



# Big Data 

## **Why single-node databases canâ€™t handle Big Data**

* **Storage limit:** Single-node systems have finite disk and memory.
* **Compute limit:** Single CPU/machine cannot efficiently process petabytes of data.
* **Fault tolerance:** Single-node failure causes complete downtime.

---

## **Problems solved by distributed systems & new challenges**

* **Solves:**

  * Horizontal scalability
  * Fault tolerance / high availability
  * Parallel processing for large datasets

* **Challenges:**

  * Network latency
  * Data consistency (CAP theorem)
  * Distributed transactions
  * Debugging and monitoring complexity

---

## **OLTP vs OLAP**

* **OLTP:**
  * Online Transaction Processing
  * Real-Time,Transactional, row-based, frequent small writes/reads, save original and standard data 
  * Example: Banking, e-commerce

* **OLAP:**
  * Online Analytical Processing 
  * Analytical, columnar, large aggregations through ETL based on OLTP
  * Example: Business intelligence, reporting

* **Design goal:** 
  * OLTP â†’ fast transactional integrity; 
  * OLAP â†’ fast analytical queries

OLTPï¼š 
- ç”¨é€”ï¼šå¤„ç†æ—¥å¸¸äº‹åŠ¡å‹æ“ä½œï¼Œå¼ºè°ƒ é«˜å¹¶å‘ã€å¿«é€Ÿå†™å…¥/æ›´æ–°/æŸ¥è¯¢å•æ¡è®°å½•
- ç‰¹ç‚¹ï¼šå®æ—¶æ€§å¼ºï¼Œè¡¨ç»“æ„é€šå¸¸è§„èŒƒåŒ–
- åœºæ™¯ï¼šé“¶è¡Œè½¬è´¦ã€è´­ç‰©ä¸‹å•ã€åº“å­˜æ›´æ–°

OLAPï¼š
- ç”¨é€”ï¼šç”¨äºåˆ†æå’Œå†³ç­–æ”¯æŒï¼Œå¼ºè°ƒ å¤æ‚æŸ¥è¯¢ã€èšåˆã€ç»Ÿè®¡
- ç‰¹ç‚¹ï¼šæŸ¥è¯¢æ…¢ä½†å¤æ‚ï¼Œè¡¨ç»“æ„é€šå¸¸æ˜Ÿå‹/é›ªèŠ±å‹ï¼Œå­˜å†å²æ•°æ®
- åœºæ™¯ï¼šé”€å”®æ•°æ®åˆ†æã€ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡ã€è´¢åŠ¡æŠ¥è¡¨

---

## **ETL vs ELT**

* **ETL:** Extract â†’ Transform â†’ Load
  * Transform before loading; good for traditional warehouses
* **ELT:** Extract â†’ Load â†’ Transform
  * Transform in data warehouse (e.g., Snowflake, Redshift); suits cloud-scale

* **Reason ELT is preferred in cloud:** Scalability, compute separation, easier schema changes

- ETL (Extract-Transform-Load)
æµç¨‹ï¼šå…ˆ æŠ½å–(Extract) â†’ è½¬æ¢(Transform) â†’ åŠ è½½(Load)
ç‰¹ç‚¹ï¼šæ•°æ®åœ¨è¿›å…¥ç›®æ ‡ä»“åº“å‰å°±å·²ç»æ¸…æ´—ã€è½¬æ¢å¥½
é€‚ç”¨åœºæ™¯ï¼šä¼ ç»Ÿæ•°æ®ä»“åº“ï¼Œç›®æ ‡åº“è®¡ç®—èƒ½åŠ›æœ‰é™
- ELT (Extract-Load-Transform)
æµç¨‹ï¼šå…ˆ æŠ½å–(Extract) â†’ åŠ è½½(Load) â†’ åœ¨ç›®æ ‡åº“è½¬æ¢(Transform)
ç‰¹ç‚¹ï¼šä¾èµ–ç›®æ ‡åº“å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›ï¼ˆå¦‚äº‘ä»“åº“ã€åˆ†å¸ƒå¼ç³»ç»Ÿï¼‰è¿›è¡Œè½¬æ¢
é€‚ç”¨åœºæ™¯ï¼šç°ä»£æ•°æ®æ¹–/äº‘ä»“åº“ï¼Œå¤§æ•°æ®åˆ†æåœºæ™¯

| ç‰¹æ€§    | ETL           | ELT           |
| ----- | ------------- | ------------- |
| è½¬æ¢ä½ç½®  | åœ¨æºæˆ–ä¸­é—´å¤„ç†       | åœ¨ç›®æ ‡ä»“åº“å¤„ç†       |
| é€‚åˆæ•°æ®é‡ | å°åˆ°ä¸­ç­‰          | å¤§æ•°æ®é‡          |
| ä¾èµ–    | ä¸­é—´æœåŠ¡å™¨æˆ– ETL å·¥å…· | ç›®æ ‡æ•°æ®åº“/ä»“åº“è®¡ç®—èƒ½åŠ›å¼º |
| æ•°æ®å»¶è¿Ÿ  | é«˜ï¼Œè½¬æ¢è€—æ—¶        | ä½ï¼Œå¯ç›´æ¥åŠ è½½åŸå§‹æ•°æ®   |

ELT Code Sampe
```python
import pandas as pd
import sqlite3

# Extract & Load
data = pd.read_csv('orders.csv')  # æŠ½å–
conn = sqlite3.connect('olap.db')
data.to_sql('orders_raw', conn, if_exists='replace', index=False)  # ç›´æ¥åŠ è½½

# Transformï¼ˆåœ¨æ•°æ®åº“é‡Œæ‰§è¡Œï¼‰
cur = conn.cursor()
cur.execute('''
UPDATE orders_raw
SET total_price = quantity * price
WHERE total_price IS NULL
''')
conn.commit()
```


---

## **MapReduce execution flow**

* **Map:** Process input splits, produce (key, value) pairs
* **Shuffle/Sort:** Group values by key, redistribute across nodes
* **Reduce:** Aggregate grouped values, write output

### 1. æ¦‚å¿µæ€»ç»“

* **MapReduce** æ˜¯å¤§æ•°æ®åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶çš„æ ¸å¿ƒç¼–ç¨‹æ¨¡å‹
* **ç›®æ ‡**ï¼šå¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼Œé€šè¿‡ **Mapï¼ˆæ˜ å°„ï¼‰** å’Œ **Reduceï¼ˆå½’çº¦ï¼‰** ä¸¤æ­¥å®Œæˆ
* **ç‰¹ç‚¹**ï¼šå¯æ‰©å±•åˆ°æˆåƒä¸Šä¸‡èŠ‚ç‚¹ï¼Œè‡ªåŠ¨å¤„ç†æ•°æ®åˆ†ç‰‡å’Œå®¹é”™

### 2. æ‰§è¡Œæµç¨‹åˆ†è§£

#### **1ï¼šInput Splittingï¼ˆè¾“å…¥åˆ†ç‰‡ï¼‰**

* å°†å¤§æ–‡ä»¶åˆ‡æˆå¤šä¸ª **Input Split**
* æ¯ä¸ª split åˆ†é…ç»™ä¸€ä¸ª **Map Task**

#### **2ï¼šMappingï¼ˆæ˜ å°„ï¼‰**

* Map å‡½æ•°å¤„ç†æ¯ä¸ª split
* **è¾“å…¥**ï¼š`<key, value>` å¯¹
* **è¾“å‡º**ï¼šä¸­é—´ `<key, value>` å¯¹

#### **3ï¼šShuffling & Sortingï¼ˆæ´—ç‰Œå’Œæ’åºï¼‰**

* ç³»ç»Ÿè‡ªåŠ¨å°† Map è¾“å‡ºæ ¹æ® key åˆ†ç»„
* ç›¸åŒ key çš„æ•°æ®ä¼šå‘é€åˆ°åŒä¸€ä¸ª Reduce Task

#### **4ï¼šReducingï¼ˆå½’çº¦ï¼‰**

* Reduce å‡½æ•°å¯¹ç›¸åŒ key çš„æ‰€æœ‰ values è¿›è¡Œå¤„ç†
* è¾“å‡ºæœ€ç»ˆç»“æœ `<key, aggregated_value>`

#### **5ï¼šOutputï¼ˆè¾“å‡ºç»“æœï¼‰**

* Reduce è¾“å‡ºå†™å…¥ HDFS æˆ–ç›®æ ‡å­˜å‚¨

### 3. å›¾ç¤ºæµç¨‹ï¼ˆæ–‡å­—ç‰ˆï¼‰

```
[Input File]
      â”‚
      â–¼
[Input Split] â†’ åˆ†é…ç»™å¤šä¸ª Map Task
      â”‚
      â–¼
[Map Function] â†’ ç”Ÿæˆä¸­é—´ <key, value>
      â”‚
      â–¼
[Shuffle & Sort] â†’ ç›¸åŒ key èšåˆ
      â”‚
      â–¼
[Reduce Function] â†’ æ±‡æ€»å¤„ç†
      â”‚
      â–¼
[Output File]
```

### 4. Python æ¨¡æ‹Ÿç¤ºä¾‹ï¼ˆWord Countï¼‰

```python
from collections import defaultdict

#   1 & 2: Map
data = ["hello world", "hello mapreduce"]
intermediate = []
for line in data:
    words = line.split()
    for word in words:
        intermediate.append((word, 1))  # Map è¾“å‡º <word, 1>

# Step 3: Shuffle & Sort
shuffle_sort = defaultdict(list)
for key, value in intermediate:
    shuffle_sort[key].append(value)

# Step 4: Reduce
result = {}
for key, values in shuffle_sort.items():
    result[key] = sum(values)  # Reduce èšåˆ

print(result)  # è¾“å‡º: {'hello': 2, 'world': 1, 'mapreduce': 1}
```

> æ ¸å¿ƒï¼šMap è´Ÿè´£æ‹†å’Œæ ‡è®°ï¼ŒShuffle æŒ‰ key èšåˆï¼ŒReduce æ±‡æ€»è®¡ç®—ã€‚


---

###### **Data Lake vs Data Warehouse**

* **Data Lake:** Raw, unstructured/semi-structured data, low cost, flexible

* **Data Warehouse:** Structured, cleaned, optimized for queries, high performance

* **Use case:** 
  * Lakes â†’ big raw data storage; 
  * Warehouse â†’ analytics & reporting

Data Lake-> Result after Loading of ELT
- Data Lake = é›†ä¸­å­˜å‚¨å„ç§åŸå§‹æ•°æ®ï¼Œä¸è¦æ±‚ç«‹å³æ¸…æ´—ã€è½¬æ¢æˆ–ç»“æ„åŒ–
- ç›®çš„æ˜¯ä¿ç•™ åŸå§‹ã€å®Œæ•´ã€å¯è¿½æº¯çš„æ•°æ®ï¼Œä»¥å¤‡åç»­åˆ†ææˆ– ETL/ELT ä½¿ç”¨
- æ•°æ®æ ¼å¼å¯ä»¥æ˜¯ï¼šCSVã€JSONã€Parquetã€æ—¥å¿—ã€å›¾ç‰‡ã€éŸ³è§†é¢‘ç­‰

Data Warehouse -> Result after Extraction of ETL 

---

###### **Batch Processing vs Streaming**

* **Batch:** Process a large size of data at once in chunks
  * Process Mode: Collect Data - Compute in Buiks - Output 
  * Characteristic
    * High Latency ( minutes to hours or even days)
    * High fault tolerance, suitable for large-scale computation 
    * Can recompute the batch, suitable for complex aggregations 
  * Use Case: Daily log analysis, reporting, ETL 
* **Streaming:** Process live data continuously as it arrives as small micro-batches 
  * Process Mode: Data arrives -> process immediately -> output 
  * Characteristic
    * Low Latency(seconds)
    * Real-time, Continuous Processing 
    * Requirement to high throughput and state management 
  * Use Case: Real-time clickstream analysis, monitoring, live logging 

* **Real-world use:** Batch â†’ daily report, Streaming â†’ live carpool tracking

| ç‰¹æ€§   | Batch Processing          | Streaming Processing                  |
| ---- | ------------------------- | ------------------------------------- |
| æ•°æ®è¾“å…¥ | å¤§å—å†å²æ•°æ®                    | å®æ—¶æµå¼æ•°æ®                                |
| å»¶è¿Ÿ   | é«˜ï¼ˆåˆ†é’Ÿ-å°æ—¶ï¼‰                  | ä½ï¼ˆæ¯«ç§’-ç§’ï¼‰                               |
| å®¹é”™   | é‡ç®—æ‰¹æ¬¡å³å¯                    | çŠ¶æ€ç®¡ç†å¤æ‚ï¼Œéœ€ checkpoint                   |
| ä½¿ç”¨åœºæ™¯ | æŠ¥è¡¨ã€å†å²åˆ†æã€ETL               | å®æ—¶ç›‘æ§ã€æ¨èã€å‘Šè­¦                            |
| è®¡ç®—ç²’åº¦ | æ‰¹æ¬¡                        | å•æ¡æˆ–å¾®æ‰¹                                 |
| æŠ€æœ¯æ ˆ  | Hadoop, Spark Batch, Hive | Spark Streaming, Flink, Kafka Streams |


---

###### **Star Schema vs Snowflake Schema**
Star schema and snowflake schema all belong to dimentional modeling methods of data warehouse. Its usage applied on BI reports like Tableau/PowerBI. It doesn't support new SQL syntax or function, but design perspective, exposed interface is still standard SQL, but inner changing structure esure easier, quicker, stabler SQL. 

It seperates tables into 2 roles/categories: fact and dimension, former one stores metrics, foreign key, latter one store business attributes like name, reducing nesting level for better analysis

Concrete Usage: 
```
ä¸šåŠ¡ç³»ç»Ÿ(OLTP)
   â†“
ETL / ELT
   â†“
ODS / Staging
   â†“
Data Warehouse/Mart Layer
   â†“
Data Mart / Semantic Layer
   â†“
BI / Analytics / DS
```
* **Star Schema:** Fact table at center, denormalized dimension tables

  * Fast for queries, simpler BI reports

* **Snowflake Schema:** Dimension tables normalized

  * Saves space, but slower joins

* **BI preference:** Star schema â†’ easier aggregations and faster query performance

| Schema               | Description                                                                                            |
| -------------------- | ------------------------------------------------------------------------------------------------------ |
| **Star Schema**      | A central fact table connected directly to denormalized dimension tables. Simple, flat design.         |
| **Snowflake Schema** | A fact table connected to normalized dimension tables, which may be further split into sub-dimensions. |

| Feature             | Star Schema                      | Snowflake Schema                               |
| ------------------- | -------------------------------- | ---------------------------------------------- |
| Dimension Tables    | Denormalized                     | Normalized (may have sub-dimensions)           |
| Complexity          | Simple, easy to understand       | More complex, hierarchical                     |
| Query Performance   | Faster (fewer joins)             | Slower (more joins required)                   |
| Storage Requirement | Higher (data redundancy)         | Lower (less redundancy)                        |
| Maintenance         | Easier                           | Harder (updates in multiple tables)            |
| Example Use Case    | OLAP dashboards, quick reporting | Data warehouses requiring strict normalization |

Star Schema 
```
       ç»´åº¦è¡¨1
          â”‚
ç»´åº¦è¡¨2 â”€â”€ äº‹å®è¡¨ â”€â”€ ç»´åº¦è¡¨3
          â”‚
       ç»´åº¦è¡¨4
```

Snowflake Schema 
```
         å­ç»´è¡¨1
            â”‚
         ç»´åº¦è¡¨1
            â”‚
äº‹å®è¡¨ â”€â”€ ç»´åº¦è¡¨2 â”€â”€ å­ç»´è¡¨2
```
é›ªèŠ±æ¨¡å¼å°†ç»´åº¦è¡¨è§„èŒƒåŒ–æ‹†åˆ†ï¼Œæ˜Ÿå‹æ¨¡å¼ç»´åº¦è¡¨æ‰å¹³åŒ–å­˜å‚¨ã€‚


## 1. What is Apache Spark, and how does it fit into the big data ecosystem (storage vs compute vs resource management)?

* Apache Spark is an **open-source distributed computing system** designed for fast, in-memory data processing.
* It sits primarily in the **compute layer** of the big data ecosystem, whereas storage is handled by systems like HDFS, S3, or other object/file stores, and resource management is handled by frameworks like YARN, Mesos, or Kubernetes.
* Spark **does not provide storage** itself; it reads and writes data from storage layers.
* It **manages distributed computation**, scheduling tasks across a cluster, optimizing execution through DAG scheduling, and handling memory and task distribution efficiently.
* Spark fits in a **storage-compute-resource separation model**:

  * **Storage**: HDFS, S3, data lakes
  * **Compute**: Spark engines executing transformations and actions
  * **Resource management**: YARN, Mesos, Kubernetes controlling cluster resources for jobs

---

## 2. Explain lazy evaluation in Spark. Why does Spark delay execution until an action is called?

* **Lazy evaluation** means Spark **does not compute results immediately when transformations are defined**. It builds a **logical plan/DAG** first.
* **Reason**:

  * Optimization: Spark can **reorder, combine, or pipeline transformations** to minimize data shuffling and disk I/O.
  * Efficient resource usage: No unnecessary computation is performed if data is not needed.
* Execution is **triggered only by an action** (e.g., `count()`, `collect()`, `save()`).
* Internally, Spark constructs a **DAG of transformations**; when an action is called, it **compiles this DAG into stages and tasks**, schedules execution, and materializes results.

> Spark used usually in data processing/transformation period of ETL/ELT, between data ingestion and lake/warehouse, the core engine in data engineering. 
> frequently used methods includes `join, filter, group by, aggregation, dedup, window, flatten/parse JSON, construction of fact table and dimension table`

---

## 3. What is the difference between transformations and actions? What happens internally when an action is triggered?

* **Transformations**: lazy operations on RDD/DataFrame/Dataset that **return a new RDD/DataFrame** without computing results immediately.

  * Examples: `map()`, `filter()`, `flatMap()`, `groupByKey()`
* **Actions**: trigger computation and **return a value or write data to storage**.

  * Examples: `collect()`, `count()`, `saveAsTextFile()`
* **Internally** when an action is called:

  1. Spark constructs a **DAG of stages** based on narrow/wide transformations.
  2. Breaks DAG into **tasks** for each partition.
  3. Schedules tasks on **executors** through the **driver**.
  4. Executes tasks and materializes the results.

---

## 4. What is an RDD? Why are immutability and partitioning important in Sparkâ€™s design?

* **RDD (Resilient Distributed Dataset)** is the **fundamental distributed data abstraction** in Spark, representing an **immutable collection of objects partitioned across the cluster**.
* **Immutability** ensures:

  * **Fault tolerance**: Spark can rebuild lost partitions from lineage.
  * **Deterministic execution**: No side effects from modifying data.
* **Partitioning** ensures:

  * **Parallelism**: Each partition can be processed independently on a separate executor.
  * **Data locality optimization**: Tasks can be scheduled where the data resides, reducing network I/O.

---

## 5. Explain the execution hierarchy in Spark: Application â†’ Job â†’ Stage â†’ Task. What determines stage boundaries?

* **Application**: The entire Spark program submitted to the cluster.
* **Job**: Triggered by an action. One action can create **one job**, multiple jobs per application.
* **Stage**: Subdivision of a job determined by **wide transformations that require shuffles**. Narrow transformations stay in the same stage.
* **Task**: Work unit for a single partition of data executed on an executor.
* **Stage boundaries**: Occur at **wide transformations** (e.g., `groupByKey`, `reduceByKey`) that need **data shuffle** across nodes.

---

## 6. What is the difference between narrow and wide transformations? Why are wide transformations usually more expensive?

* **Narrow transformations**: Each output partition depends on **one input partition**.

  * Examples: `map()`, `filter()`, `union()`
  * No shuffle required â†’ can be pipelined â†’ cheaper.
* **Wide transformations**: Each output partition depends on **multiple input partitions**.

  * Examples: `reduceByKey()`, `groupByKey()`, `join()`
  * Require **shuffle of data across the network**, sorting, and repartitioning â†’ more expensive in CPU, network, and disk I/O.

---

## 7. Explain the roles of Driver and Executor. What kind of work happens on each side?

* **Driver**:

  * Orchestrates the application.
  * **Tasks**: builds DAG, schedules jobs and stages, maintains metadata, collects results.
* **Executor**:

  * Worker process running on cluster nodes.
  * **Tasks**: executes transformations and actions on partitions, stores RDD/DataFrame blocks in memory or disk, returns results to driver.

---

## 8. What is the Medallion Architecture (Bronze, Silver, Gold)? What type of data and logic belongs in each layer?

* **Bronze**: Raw, ingested data from source systems. Minimal transformation, may contain duplicates or errors.
* **Silver**: Cleaned, normalized data. Remove duplicates, fix schema, enrich with joins. Ready for analytics.
* **Gold**: Aggregated, curated, high-value datasets optimized for dashboards, machine learning, or reporting.

---

## 9. Transformation vs Action in Spark

* **Transformations (lazy)**:

  * `map()`, `filter()`, `flatMap()`, `groupByKey()`, `join()`
  * Build DAG, no immediate execution

* **Actions (trigger computation)**:

  * `collect()`, `count()`, `take(n)`, `saveAsTextFile()`, `foreach()`

---

## ğŸ”´10. Why spark.read.csv() loads all columns as StringType by default? What problems can this cause in aggregation or filtering?

* Defaulting to `StringType` avoids schema inference overhead.
* Problems:

  * Numeric aggregation fails or produces wrong results (`"10" + "5" = "105"`).
  * Filtering may yield unexpected results (`"2" > "10"` lexicographically).
* Mitigation: use `inferSchema=True` or explicitly define schema.

---

## 11. What does explode() do in Spark? In what scenario can explode() cause serious performance issues?

* `explode()` **flattens arrays or maps into multiple rows**.
* Performance issues occur when:

  * Input contains **large nested arrays** â†’ explosion leads to **many more rows** â†’ memory and shuffle blowup.
  * Can cause **data skew** if some rows explode much more than others.

---

## 12. Why is groupBy() considered a wide transformation? What happens under the hood when a wide transformation is executed?

* **groupBy()**: output depends on multiple input partitions â†’ requires **shuffle across nodes**.
* Under the hood:

  1. Partitions are hashed or range-partitioned.
  2. Data is shuffled over the network to target partitions.
  3. Aggregation occurs per partition.
  4. Results combined and sent back to next stage.

---

## 13. Why is Amazon S3 considered Object Storage instead of a file system?

* S3 stores data as **objects** (key + value + metadata) rather than hierarchical files.
* No POSIX-compliant operations (no true directories, appending files).
* Optimized for **massive scale, durability, and availability**.

---

## 14. Explain the relationship between Bucket, Object, and Key in S3.

* **Bucket**: top-level container for objects.
* **Object**: the actual data stored, including payload and metadata.
* **Key**: unique identifier of the object within a bucket (like a â€œpathâ€).
* S3 address: `s3://bucket-name/key`

---

## 15. Why canâ€™t you update a single row inside a CSV file stored in S3?

* S3 is **object storage**, not a file system.
* Objects are **immutable**; updating a row requires rewriting the whole object.
* No random write capability like HDFS or RDBMS.

---

## 16. What is an S3 prefix, and why is it important for Spark / Hive performance?

* Prefix is the **string before the object key** acting like a â€œdirectoryâ€ for object grouping.
* Spark/Hive optimize **parallel reads** by scanning prefixes.
* Proper prefixing allows **efficient partition pruning**, reduces full-bucket scans, and improves query performance.

---

## 17. Compare IAM Policy, Bucket Policy, and Access Point Policy.

* **IAM Policy**: attached to **users/roles**; defines what AWS resources a user can access.
* **Bucket Policy**: attached to **S3 bucket**; controls which principals can access objects in the bucket level, valid for all accessing port 
* ğŸ”´**Access Point Policy**: attached to **S3 Access Point**; fine-grained control per access point for multi-tenant use cases, allows different group or apps access same bucket through different access point with independent authority

---

## 18. What problem does S3 Versioning solve, and what new problem does it introduce?

* **Solves**: accidental deletion or overwrite; keeps **all versions of an object**.
* **Introduces**: increased **storage cost**, potential complexity in managing multiple versions, must explicitly delete old versions if not needed.

## 19. What are the 4 Vs of Big Data with Sample? 
- Volume
  - Large Datasize Storage in HDFS or Cloud service like S3 in AWS
- Velocity
  - Real-time and High performance and throughput of IO between distributed replicas nodes, required high speed and immediate management 
- Variety
  - Various origin of data source, required large storage and multiple strategy to deal with 
- Veracity 
  - Lots of troublesome format problems or imcompatible arithmetic operations, required various methods and functions to figure out solution. Like parse ETL pipeline before constructing data warehouse 

## 20. Difference between Distributed Storage VS Distributed Computation with Sample 
- Former one store large sets of data in distributed replicas through splits, parginations or shardings, for data versioning control and avoid single-node failure, like Hadoop distributed ecosystem 
- Latter one store data and emphasize concurrently dealing with asynchronous tasks simutaneously, for boost efficiency of multi-threading pipeline and avoid event loop blocking. Like Spark in Hadoop eco. 

## 21. Explain NameNode and DataNode of HDFS storing large data. Why is replications important? 
HDFS refers to Hadoop Distributed File System, it applied through flexible key-value system within system, key maps to NameNode and value refer to DataNode. 

Replications are significant cuz:
1. Fault Tolerence: which support architecture to avoid crushing due to single-node failure 
2. Reduce Workloads of single server: allocate CPU computation stress to other replicas to avoid limitation of single DB/server 

## 22. Explain how MapReduce works at a high level. What problem was MapReduce design?
1. Workflow Procedure
   1. Import files and splits
   2. Map extraction of separates into relative replica nodes then merge values through map_id (?)
   3. Reduce/aggregate values in same id/group/replica nodes to analyze required features 
2. Deal with large size distributed data to support a standard procedures of whole computation

## 23. List three major differences between MapReduce and Spark.Why did Spark largely replace MapReduce in modern data pipelines?
1.  MapReduce is a computation model which applied as core concept/strategy for distributed computation system, like hadoop eco, spark, etc. 
2.  Spark support more complicated feature like batch processing, data analysis for machine learning, extention of MapReduce 
3. Spark is more derived on MapReduce, an extention of pipeline 

- Spark has integrated with multiple features with external platform like datadog, snowflake, scale computation capabilities to large industry data engineering 


## 24. What is lazy evaluation in Spark? Why is it important for performance and optimization?

- Lazy evaluation refers to schedule transformation initially then trigger actions to start data computation via terminal function like `collect(), sum()`, etc, which will finally touch eventual consistency. 
- It only trigger action when transformation schedule is ready and data is clearly designed, which largely reduce waste of computation resource. 

## 25. What is the difference between a narrow transformation and a wide transformation? Give one example of each.
- Narrow one means implement data output through single data group. like `filter(), map(), sum()` etc. 
- Wide one refers to multiple data input more. like `groupById(), reduceById()`. 

## 26. In an AWS-based data platform:
- What service is commonly used as a Data Lake?
  - S3  
- Where does Spark usually run?
  - usually run in one bucket with bucket policy and relative access point policy 
- Where is processed data typically stored for analytics?
  - usually stored in object class, which does not like hdfs or hdive, access by attributes


## 27. What is an EC2 instance in simple terms?

**Answer:**
An **EC2 instance** is a **virtual computer (server)** in AWS that you can rent and use to run your applications.

### Key idea

* Itâ€™s like getting a computer in the cloud.
* You choose its CPU, memory, storage, and OS.
* You can start it, stop it, resize it, or delete it anytime.

### Example

Running a Python Flask API:

* You create an EC2 instance.
* Install Python and your app.
* Users access your app via the instanceâ€™s public IP.



## 28. What is the difference between vertical scaling and horizontal scaling?

**Answer:**

* **Vertical scaling** = make one machine bigger.
* **Horizontal scaling** = add more machines.

### Vertical scaling (scale up/down)

Increase resources of a single server.

* Add more CPU, RAM, disk.
* Limited by max size of one machine.
* Often needs restart.

**Example:**
Change EC2 from `t3.small` (2GB RAM) â†’ `t3.large` (8GB RAM).



### Horizontal scaling (scale out/in)

Add or remove servers.

* Multiple machines work together.
* No hard upper limit.
* Better fault tolerance.

**Example:**
Go from 2 EC2 instances â†’ 10 EC2 instances behind a Load Balancer.



### Quick comparison

| Aspect          | Vertical       | Horizontal       |
| --------------- | -------------- | ---------------- |
| One machine     | Bigger         | More machines    |
| Limit           | Hardware limit | Almost unlimited |
| Fault tolerance | Low            | High             |
| Downtime        | Often yes      | Usually no       |



## 29. Why do big data systems prefer horizontal scaling?

**Answer:**
Because big data needs to process **huge volumes of data in parallel**, and horizontal scaling allows that efficiently and cheaply.

### Core reasons

1. **Data is massive**

   * Too big for one machineâ€™s disk or memory.

2. **Parallel processing**

   * Split data across nodes.
   * Process chunks at the same time.

3. **Cost-effective**

   * Many cheap servers vs one super-expensive server.

4. **Fault tolerance**

   * If one node fails, others continue.

5. **Elastic growth**

   * Add nodes as data grows.

### Example

Hadoop / Spark:

* Store data across many nodes.
* Each node processes part of the dataset.
* Results are combined.



## 30. If you terminate an EC2 instance, why might AWS still charge you?

**Answer:**
Because some **attached resources are billed separately** and may still exist after the instance is gone.

### Common reasons

1. **EBS volumes not deleted**

   * Root or data disks may remain.
   * EBS storage is charged per GB-month.

**Example:**
You terminate EC2, but its 50GB EBS volume is still there â†’ you pay for 50GB.

2. **Elastic IP (EIP) not released**

   * A static public IP.
   * Charged when allocated but not attached to a running instance.

3. **Snapshots / AMIs**

   * Backups stored in S3.
   * Charged for storage.

4. **Load balancers / other services**

   * If created for that EC2, they keep running and billing.

### Key principle

> Terminating EC2 stops compute charges, but **does not automatically delete all related resources**.
