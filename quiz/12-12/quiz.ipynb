{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eef10fe",
   "metadata": {},
   "source": [
    "Q1. Fine-tuning vs. RAG\n",
    "In your own words:\n",
    "1. What is fine-tuning in the context of LLMs? Describe what changes (or is learned) during fine-tuning.\n",
    "   1. fine tuning is similiar with RAG pipeline, but it will focus on specific and accurated datasets and need more cpu computation for specific model pretraining assigning, depending on pretrained data source in concrete direction, more memory-used and cost\n",
    "   2. RAG is more based on instantly vector embedding from external files input and RNN prompt \n",
    "2. What is Retrieval-Augmented Generation (RAG), and how is it different from fine-tuning in terms of where the knowledge lives and how it is updated?\n",
    "   1. RAG relies on external data source not just inserted LLM system, like pdfs, webpages, docs, etc\n",
    "   2. RAG uses external data as context and reference, but fine-tuning needs more specific model training, cost more computation\n",
    "   3. In RAG, knowledge lives in context window and vector knowledge, updated from recursivly prompt and generated LLM answer. \n",
    "3. Give one realistic example where you would prefer RAG over fine-tuning, and one example where you would prefer fine-tuning over RAG. Explain your reasoning briefly for each.\n",
    "   1. Prefer RAG when I needs more general summarization or researching suggestion or suggestion sys, etc \n",
    "   2. Prefer Fine-tuning when I needs specifc knowlege support, like teaching tutoring, finance master, etc. \n",
    "\n",
    "Q2. Document → Page → Block → Chunk\n",
    "You are building a RAG system for complex PDFs (e.g., annual reports).\n",
    "1. Describe the typical hierarchy Document → Page → Block → Chunk. What\n",
    "does each level represent?\n",
    "   1. Doc -> Whole report\n",
    "   2. Page -> report is seperated into a series pages during parsing depending on page number \n",
    "   3. Block -> Page is seperated based on Paragraph or Sessions, etc, but structure more boundaried and well-componneted, includes ids, headings, pages_numbers, tables fields, etc. \n",
    "   4. Chunk -> Metric which seperate Block into fixed or semantic length of tokens, providing context to prompt of LLM \n",
    "1. List at least three possible block_type values (e.g., heading, paragraph, table,figure) and explain how you might treat each one differently during indexing. \n",
    "   1. heading: indexing as dictionary id to help query \n",
    "   2. paragraph: embeding into tokens and saved in vector db  \n",
    "   3. table: indexing as nested dictionary ??? \n",
    "\n",
    "Q3. Chunking Strategy & Trade-offs\n",
    "Suppose you need to index a 150-page technical report with text, tables and diagrams.\n",
    "1. Compare fixed-size overlapping chunks (e.g., 500 tokens with 100 overlap) vs. structure-aware chunks (split by headings/sections).\n",
    "   1. the overlapping chunks will support relationship between each chunk and avoid context loss\n",
    "2. Give one example of how bad chunking could cause (a) missing important context or (b) hallucinated answers.\n",
    "   1. if chunking size is too small, context window will be squeezed, which cause context data loss, the answer from LLM will show less nearest\n",
    "3. How would you choose chunk size and overlap in this scenario, and what trade-offs are you making between recall, precision, cost and latency?\n",
    "   1. larger chunk size, more tokens used and latency \n",
    "   2. larger overlap, more related between chunks and less data loss \n",
    "   3. if I choose, usually 500-800 chunk size and 150 overlap\n",
    "\n",
    "Interview-Style Questions 1\n",
    "\n",
    "Q4. LangChain RAG with LCEL\n",
    "LangChain’s LCEL often uses patterns like:\n",
    "```\n",
    "{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "| prompt\n",
    "| llm\n",
    "| parser\n",
    "```\n",
    "1. Explain what each part of this chain is doing: retriever ,format_docs , RunnablePassthrough , prompt , llm , parser .\n",
    "   1. retriever: data source url\n",
    "   2. format_docs: [pdf, txt, etc]\n",
    "   3. RunnablePassthrough(): A passthrough function simply receives input and returns it unchanged, enabling pipelines to connect steps without modifying the data.\n",
    "   4. prompt: embedding vector list from user\n",
    "   5. llm: selected LLM model \n",
    "   6. parser: selected file parser from python lib \n",
    "2. Map these components to the standard RAG steps: retrieve → prepare context → prompt → generate → post-process.\n",
    "   1. retriever -> retrieve \n",
    "   2. format_docs -> retrieve \n",
    "   3. llm -> prompt \n",
    "   4. parser -> prepare, retrieve \n",
    "\n",
    "Q5. Tool Calling & When Not to Use Agents\n",
    "Answer the following:\n",
    "1. Briefly describe the five main steps of tool calling / function calling in an LLM system (from tool definitions to final answer).\n",
    "   1. Define tools, decide which to call, format input, call tool into model response, and return final answer to user.\n",
    "2. What is the difference between a Chain and an Agent in LangChain or similar frameworks?\n",
    "   1. Chain is a fixed sequence of steps, combined procedures in a streaming workflow\n",
    "   2. and Agent decides dynamically which tool or step to use based on the input automatically, integrated with external command, tools, files, context and more choice for model training for token embedding \n",
    "\n",
    "Q6. Evaluating a RAG or Agentic System\n",
    "You’ve built a RAG or Agentic RAG system and now need to evaluate it.\n",
    "1. Name and describe three evaluation dimensions that are especially important for RAG (for example: retrieval relevance, answer relevance,faithfulness/grounding).\n",
    "   1. retrieval relavance: measures how well the retriever selects the right chunks.\n",
    "   2. answer relevance: measures how nearest the vector list of prompt and vector context from Vector DB \n",
    "2. For open-ended answers (free-text), why is LLM-as-Judge often more useful than simple string-overlap metrics like ROUGE or BLEU?\n",
    "   1. ❌ \n",
    "3. Give one example of how you might combine automatic evaluation and  lightweight human evaluation in a realistic workflow.\n",
    "   1. for embedding, I will use pretrained model to automate several metrics which will help me to select when to use contextual embedding and when to use token embedding, the metrics are answer relevance, multi-meaning words percentage, etc \n",
    "\n",
    "Section B – Coding / Practical\n",
    "\n",
    "Interview-Style Questions 2\n",
    "Q7. Chunkization Logic from Blocks to Chunks\n",
    "You receive a parsed document as a list of blocks, for example:\n",
    "```\n",
    "blocks = [\n",
    " {\"block_id\": 1, \"block_type\": \"heading\", \"text\": \"1. Introduction\"},\n",
    " {\"block_id\": 2, \"block_type\": \"paragraph\", \"text\": \"This report discusses\n",
    "...\"},\n",
    " {\"block_id\": 3, \"block_type\": \"paragraph\", \"text\": \"Our main contributions ar\n",
    "e ...\"},\n",
    " {\"block_id\": 4, \"block_type\": \"heading\", \"text\": \"2. Methods\"},\n",
    " {\"block_id\": 5, \"block_type\": \"paragraph\", \"text\": \"We collected data from\n",
    "...\"},\n",
    " # ...\n",
    "]\n",
    "```\n",
    "Design a simple chunkization function in Python-like pseudo-code:\n",
    "```\n",
    "def make_chunks(blocks, max_tokens: int = 300, overlap_tokens: int = 50):\n",
    " \"\"\"\n",
    " Returns a list of chunks, where each chunk is a dict like:\n",
    " {\n",
    " \"chunk_id\": int,\n",
    " \"text\": \"...\",\n",
    " \"source_block_ids\": [ ... ],\n",
    " \"heading_path\": [\"1. Introduction\", ...],\n",
    " }\n",
    " \"\"\"\n",
    "```\n",
    "Requirements / hints:\n",
    "1. Try to respect headings: when possible, avoid mixing content from different top-level sections in the same chunk.\n",
    "2. Within a section, concatenate paragraph blocks until you are near max_tokens (you may approximate token count by word count).\n",
    "3. Implement simple overlap between neighboring chunks (e.g., last overlap_tokens from previous chunk appear at the beginning of the next). You do not need to call a real tokenizer; you can approximate with len(text.split()) .\n",
    "Focus on the logic and metadata, not exact code correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(blocks, max_tokens: int = 300, overlap_tokens: int = 50):\n",
    "    \"\"\"\n",
    "    psudo-code\n",
    "    \n",
    "    start = 0\n",
    "    dic = {}\n",
    "    text_length = len([block.text for block in blocks].parse())\n",
    "    cur = 0\n",
    "    chunk_id = 0\n",
    "    chunk_text = \"\"\n",
    "    source_ids = []\n",
    "    heading_path = []\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = start + max_tokens - overlap_tokens\n",
    "        if end > text_length:\n",
    "            end = text_length\n",
    "        while cur < end:\n",
    "            chunk_text += text_tokens[cur] + \" \"\n",
    "            bid = find_block_id(cur, blocks)\n",
    "            cur += 1\n",
    "        dic = {\n",
    "            \"chunk_id\": chunk_id, \n",
    "            \"text\": chunk_text.strip(), \n",
    "            \"source_block_ids\": source_ids, \n",
    "            \"heading_path\": heading_path\n",
    "            }\n",
    "        chunks.append(dic)\n",
    "        chunk_id += 1\n",
    "        overlap_start = max(start, end - overlap_tokens)\n",
    "        start = overlap_start\n",
    "        chunk_text = \" \".join(text_tokens[start:end]) + \" \"\n",
    "        source_ids = []\n",
    "    return chunks\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e33883",
   "metadata": {},
   "source": [
    "Q8. Minimal RAG And Tool-Calling Chain\n",
    "Minimal RAG pipeline using any vector store\n",
    "\n",
    "Write Python for a minimal RAG flow using a high-level vector store API (no need to implement similarity search yourself):\n",
    "1. Indexing step:\n",
    "```\n",
    "from some_embedding_lib import embed\n",
    "from some_vector_store import VectorStore # e.g., FAISS/Chroma-like AP\n",
    "I\n",
    "docs = [\n",
    " {\"doc_id\": \"doc-1\", \"text\": \"....\"},\n",
    " {\"doc_id\": \"doc-2\", \"text\": \"....\"},\n",
    "]\n",
    "# 1) split docs into chunks (you may reuse your make_chunks() idea)\n",
    "# 2) create embeddings for each chunk\n",
    "# 3) add them to vector_store with metadata (doc_id, chunk_id, heading, e\n",
    "tc.)\n",
    "```\n",
    "2. Query step (per question):\n",
    "```\n",
    "def answer_question(question: str) -> str:\n",
    " # a) embed the question\n",
    " # b) vector_store.similarity_search(question_embedding, k=5)\n",
    " # c) format retrieved chunks into a context string\n",
    " # d) build a prompt:\n",
    " # \"Use ONLY the context below to answer the question...\n",
    " # Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    " # e) call llm(prompt) and return the answer\n",
    "```\n",
    "Your answer should show the data flow clearly: documents → chunks →\n",
    "embeddings → vector_store → retrieval → prompt → LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee81f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Psedo code \n",
    "\n",
    "from some_embedding_lib import embed\n",
    "from some_vector_store import VectorStore # e.g., FAISS/Chroma-like API\n",
    "from make_chunks import make_chunks\n",
    "\n",
    "docs = [\n",
    " {\"doc_id\": \"doc-1\", \"text\": \"....\"},\n",
    " {\"doc_id\": \"doc-2\", \"text\": \"....\"},\n",
    "]\n",
    "\n",
    "chunks = []\n",
    "for doc in docs:\n",
    "    doc_chunks = make_chunks(doc[\"text\"])\n",
    "    for c in doc_chunks:\n",
    "        c.update({\"doc_id\": doc[\"doc_id\"]})\n",
    "        chunks.append(c)\n",
    "\n",
    "vector_store = VectorStore()\n",
    "\n",
    "for c in chunks:\n",
    "    c_embedding = embed(c[\"text\"])\n",
    "    vector_store.add_vector(\n",
    "        c_embedding, \n",
    "        metadata={\n",
    "            \"doc_id\": c[\"doc_id\"], \n",
    "            \"chunk_id\": c[\"chunk_id\"], \n",
    "            \"heading_path\": c[\"heading_path\"]\n",
    "            })\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5b9b0",
   "metadata": {},
   "source": [
    "Simple LangChain tool-calling chain\n",
    "\n",
    "Design a small LangChain-based tool-calling pipeline with one tool, e.g. a fake exchange-rate lookup:\n",
    "```\n",
    "from langchain_core.tools import tool\n",
    "RATES = {(\"USD\", \"EUR\"): 0.9, (\"EUR\", \"USD\"): 1.1}\n",
    "@tool\n",
    "def get_exchange_rate(base: str, quote: str) -> float:\n",
    " \"\"\"Get the FX rate from base currency to quote currency.\n",
    " Use this when the user asks to convert or compare currencies.\"\"\"\n",
    " return RATES[(base, quote)]\n",
    "```\n",
    "Tasks:\n",
    "1. Show how you would bind this tool to a chat model (e.g., ChatOpenAI ) in\n",
    "LangChain and create a tool-aware LLM.\n",
    "2. Write the control flow for one full interaction:\n",
    "```\n",
    "user_message = \"If I have 100 USD, how many EUR is that approximatel\n",
    "y?\"\n",
    "# a) call llm_with_tools.invoke(user_message)\n",
    "# b) inspect response.tool_calls\n",
    "# c) execute the requested tool(s) with the given args\n",
    "# d) send a follow-up message to the model with the tool results\n",
    "# e) return the final natural-language answer to the user\n",
    "```\n",
    "\n",
    "You may omit import boilerplate, but the sequence of steps and the division between model and application code should be clear."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
